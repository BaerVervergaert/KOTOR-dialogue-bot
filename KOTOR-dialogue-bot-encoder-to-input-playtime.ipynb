{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef69c6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from math import ceil, floor\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import urllib\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "691d77dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Config settings\n",
    "\n",
    "## Data config\n",
    "# Download the data again regardless if it already exists?\n",
    "download_again = False\n",
    "\n",
    "\n",
    "## DataLoader config\n",
    "# Batch size\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "## Model config\n",
    "# filenames for model storage\n",
    "encoder_name = 'kotor-rnn-encoder-encinput.ptm'\n",
    "decoder_name = 'kotor-rnn-decoder-encinput.ptm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f55a61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "use_cuda = False\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "print(device)\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69224d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://github.com/hmi-utwente/video-game-text-corpora/raw/master/Star%20Wars:%20Knights%20of%20the%20Old%20Republic/data/dataset_20200716.csv'\n",
    "filename = 'dataset_20200716.csv'\n",
    "if (not os.path.exists(filename)) or download_again:\n",
    "    urllib.request.urlretrieve(url,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69a2f005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker</th>\n",
       "      <th>listener</th>\n",
       "      <th>text</th>\n",
       "      <th>animation</th>\n",
       "      <th>comment</th>\n",
       "      <th>next</th>\n",
       "      <th>previous</th>\n",
       "      <th>source_dlg</th>\n",
       "      <th>audiofile</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anchorhead Tradesman</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Take care of yourself. The price of kolto tank...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[None]</td>\n",
       "      <td>tat17_news_01</td>\n",
       "      <td>NM17AANEWS11000_.mp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Anchorhead Tradesman</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Selkath put a bunch of export restrictions...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[None]</td>\n",
       "      <td>tat17_news_01</td>\n",
       "      <td>NM17AANEWS11001_.mp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Anchorhead Tradesman</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I hear that Manaan is no longer shipping kolto...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[None]</td>\n",
       "      <td>tat17_news_01</td>\n",
       "      <td>NM17AANEWS11002_.mp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Anchorhead Tradesman</td>\n",
       "      <td>NaN</td>\n",
       "      <td>If you have kolto tanks, use them sparingly. I...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[None]</td>\n",
       "      <td>tat17_news_01</td>\n",
       "      <td>NM17AANEWS11003_.mp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Anchorhead Tradesman</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm sure I saw some holo-footage of you on the...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[None]</td>\n",
       "      <td>tat17_news_01</td>\n",
       "      <td>NM17AANEWS11004_.mp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29208</th>\n",
       "      <td>Zaalbar</td>\n",
       "      <td>NaN</td>\n",
       "      <td>It is a description of the ritual you have alr...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[29207, 29211]</td>\n",
       "      <td>kas25_ritualmark</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29209</th>\n",
       "      <td>Zaalbar</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I never went to the Shadowlands to prove mysel...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[29210]</td>\n",
       "      <td>[29207, 29211]</td>\n",
       "      <td>kas25_ritualmark</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29210</th>\n",
       "      <td>Zaalbar</td>\n",
       "      <td>NaN</td>\n",
       "      <td>You will have to follow whatever your instinct...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[29209]</td>\n",
       "      <td>kas25_ritualmark</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29211</th>\n",
       "      <td>Player</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Whatever. Just tell me what you know about it.</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[29208, 29209]</td>\n",
       "      <td>[29206]</td>\n",
       "      <td>kas25_ritualmark</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29212</th>\n",
       "      <td>Conversation owner</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Obviously this was once a place of great ritu...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[29205]</td>\n",
       "      <td>kas25_ritualmark</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29213 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    speaker listener  \\\n",
       "id                                     \n",
       "0      Anchorhead Tradesman      NaN   \n",
       "1      Anchorhead Tradesman      NaN   \n",
       "2      Anchorhead Tradesman      NaN   \n",
       "3      Anchorhead Tradesman      NaN   \n",
       "4      Anchorhead Tradesman      NaN   \n",
       "...                     ...      ...   \n",
       "29208               Zaalbar      NaN   \n",
       "29209               Zaalbar      NaN   \n",
       "29210               Zaalbar      NaN   \n",
       "29211                Player      NaN   \n",
       "29212    Conversation owner      NaN   \n",
       "\n",
       "                                                    text animation comment  \\\n",
       "id                                                                           \n",
       "0      Take care of yourself. The price of kolto tank...        []     NaN   \n",
       "1      The Selkath put a bunch of export restrictions...        []     NaN   \n",
       "2      I hear that Manaan is no longer shipping kolto...        []     NaN   \n",
       "3      If you have kolto tanks, use them sparingly. I...        []     NaN   \n",
       "4      I'm sure I saw some holo-footage of you on the...        []     NaN   \n",
       "...                                                  ...       ...     ...   \n",
       "29208  It is a description of the ritual you have alr...        []     NaN   \n",
       "29209  I never went to the Shadowlands to prove mysel...        []     NaN   \n",
       "29210  You will have to follow whatever your instinct...        []     NaN   \n",
       "29211     Whatever. Just tell me what you know about it.        []     NaN   \n",
       "29212  [Obviously this was once a place of great ritu...        []     NaN   \n",
       "\n",
       "                 next        previous        source_dlg             audiofile  \n",
       "id                                                                             \n",
       "0                  []          [None]     tat17_news_01  NM17AANEWS11000_.mp3  \n",
       "1                  []          [None]     tat17_news_01  NM17AANEWS11001_.mp3  \n",
       "2                  []          [None]     tat17_news_01  NM17AANEWS11002_.mp3  \n",
       "3                  []          [None]     tat17_news_01  NM17AANEWS11003_.mp3  \n",
       "4                  []          [None]     tat17_news_01  NM17AANEWS11004_.mp3  \n",
       "...               ...             ...               ...                   ...  \n",
       "29208              []  [29207, 29211]  kas25_ritualmark                   NaN  \n",
       "29209         [29210]  [29207, 29211]  kas25_ritualmark                   NaN  \n",
       "29210              []         [29209]  kas25_ritualmark                   NaN  \n",
       "29211  [29208, 29209]         [29206]  kas25_ritualmark                   NaN  \n",
       "29212              []         [29205]  kas25_ritualmark                   NaN  \n",
       "\n",
       "[29213 rows x 9 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_col = 'id'\n",
    "usecols = None\n",
    "#usecols = ['id','text','previous']\n",
    "converters = {'previous':ast.literal_eval,\n",
    "              'next':ast.literal_eval,\n",
    "             }\n",
    "data = pd.read_csv(filename,\n",
    "                   index_col=index_col,\n",
    "                   usecols=usecols,\n",
    "                   converters=converters,\n",
    "                  )\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60895540",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "speaker       object\n",
       "listener      object\n",
       "text          object\n",
       "animation     object\n",
       "comment       object\n",
       "next          object\n",
       "previous      object\n",
       "source_dlg    object\n",
       "audiofile     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7462385b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 29213 entries, 0 to 29212\n",
      "Data columns (total 9 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   speaker     29213 non-null  object\n",
      " 1   listener    2231 non-null   object\n",
      " 2   text        29213 non-null  object\n",
      " 3   animation   29213 non-null  object\n",
      " 4   comment     2658 non-null   object\n",
      " 5   next        29213 non-null  object\n",
      " 6   previous    29213 non-null  object\n",
      " 7   source_dlg  29213 non-null  object\n",
      " 8   audiofile   12325 non-null  object\n",
      "dtypes: object(9)\n",
      "memory usage: 2.2+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed7e886a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t speaker\n",
      "count    29213.000000\n",
      "mean         8.973094\n",
      "std          4.522340\n",
      "min          3.000000\n",
      "25%          6.000000\n",
      "50%          6.000000\n",
      "75%         12.000000\n",
      "max         24.000000\n",
      "Name: speaker, dtype: float64\n",
      "\n",
      "\t text\n",
      "count    29213.000000\n",
      "mean        82.780748\n",
      "std         49.850241\n",
      "min          1.000000\n",
      "25%         39.000000\n",
      "50%         74.000000\n",
      "75%        122.000000\n",
      "max        344.000000\n",
      "Name: text, dtype: float64\n",
      "\n",
      "\t animation\n",
      "count    29213.000000\n",
      "mean         4.144867\n",
      "std          8.150476\n",
      "min          2.000000\n",
      "25%          2.000000\n",
      "50%          2.000000\n",
      "75%          2.000000\n",
      "max        243.000000\n",
      "Name: animation, dtype: float64\n",
      "\n",
      "\t next\n",
      "count    29213.000000\n",
      "mean         1.498100\n",
      "std          1.451002\n",
      "min          0.000000\n",
      "25%          1.000000\n",
      "50%          1.000000\n",
      "75%          2.000000\n",
      "max         22.000000\n",
      "Name: next, dtype: float64\n",
      "\n",
      "\t previous\n",
      "count    29213.000000\n",
      "mean         1.611235\n",
      "std          1.664769\n",
      "min          1.000000\n",
      "25%          1.000000\n",
      "50%          1.000000\n",
      "75%          1.000000\n",
      "max         34.000000\n",
      "Name: previous, dtype: float64\n",
      "\n",
      "\t source_dlg\n",
      "count    29213.000000\n",
      "mean        13.488344\n",
      "std          1.975279\n",
      "min          2.000000\n",
      "25%         12.000000\n",
      "50%         13.000000\n",
      "75%         15.000000\n",
      "max         16.000000\n",
      "Name: source_dlg, dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for col in data.columns:\n",
    "    try:\n",
    "        desc = data[col].apply(len).describe()\n",
    "        print('\\t',col)\n",
    "        print(desc)\n",
    "        print()\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fae13ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_start_stop_codon(data,column,start='\\r',stop='\\n',force=False):\n",
    "    detect_start_stop = lambda s,start=start,stop=stop: start in s or stop in s\n",
    "    codons_in_text = data[column].apply(detect_start_stop).any()\n",
    "    if codons_in_text:\n",
    "        if not force:\n",
    "            raise ValueError('data already contains start or stop codon at column: {0}'.format(column))\n",
    "    transform = lambda s,start=start,stop=stop: start+s+stop\n",
    "    data[column] = data[column].apply(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71c48f28",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "0        \\rTake care of yourself. The price of kolto ta...\n",
       "1        \\rThe Selkath put a bunch of export restrictio...\n",
       "2        \\rI hear that Manaan is no longer shipping kol...\n",
       "3        \\rIf you have kolto tanks, use them sparingly....\n",
       "4        \\rI'm sure I saw some holo-footage of you on t...\n",
       "                               ...                        \n",
       "29208    \\rIt is a description of the ritual you have a...\n",
       "29209    \\rI never went to the Shadowlands to prove mys...\n",
       "29210    \\rYou will have to follow whatever your instin...\n",
       "29211    \\rWhatever. Just tell me what you know about i...\n",
       "29212    \\r[Obviously this was once a place of great ri...\n",
       "Name: text, Length: 29213, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_start_stop_codon(data,'text')\n",
    "data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd27d21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset:\n",
    "    def __init__(self,data):\n",
    "        self.data = data\n",
    "        self.tensors = {}\n",
    "        \n",
    "        #Make Vocab\n",
    "        self.vocab = sorted(list(set(''.join(self.data['text']))))\n",
    "        self.ch2i = { v:k for k,v in enumerate(self.vocab) }\n",
    "        self.i2ch = { v:k for k,v in self.ch2i.items() }\n",
    "        \n",
    "    def __len__(self):\n",
    "        return(len(self.data))\n",
    "    def str2vec(self,text):\n",
    "        out = torch.tensor([ self.ch2i[s] for s in text],dtype=torch.long)\n",
    "        return(out)\n",
    "    def vec2str(self,vec):\n",
    "        out = ''.join([ self.i2ch[i.item()] for i in vec ])\n",
    "        return(out)\n",
    "    def get_dialogue(self,idx):\n",
    "        try:\n",
    "            dialogue = self.tensors[idx]\n",
    "        except:\n",
    "            if idx is None:\n",
    "                dialogue = '\\r\\n'\n",
    "            else:\n",
    "                dialogue = self.data.loc[idx,'text']\n",
    "            dialogue = self.str2vec(dialogue)\n",
    "            self.tensors[idx] = dialogue\n",
    "        return(dialogue)\n",
    "    def __getitem__(self,idx):\n",
    "        self.data.loc[idx,'text']\n",
    "        response = self.get_dialogue(idx)\n",
    "        ins = response[:-1]\n",
    "        outs = response[1:]\n",
    "        prevs = self.data.loc[idx,'previous']\n",
    "        prevs = np.random.choice(prevs)\n",
    "        prevs = self.get_dialogue(prevs)\n",
    "        return(prevs,ins,outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90bfcb65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29213\n",
      "\n",
      "['\\n', '\\r', ' ', '!', '\"', '#', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '^', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "\n",
      "(tensor([1, 0]), tensor([ 1, 53, 64, 74, 68,  2, 66, 64, 81, 68,  2, 78, 69,  2, 88, 78, 84, 81,\n",
      "        82, 68, 75, 69, 15,  2, 53, 71, 68,  2, 79, 81, 72, 66, 68,  2, 78, 69,\n",
      "         2, 74, 78, 75, 83, 78,  2, 83, 64, 77, 74, 82,  2, 71, 64, 82,  2, 73,\n",
      "        84, 76, 79, 68, 67,  2, 83, 71, 81, 78, 84, 70, 71,  2, 83, 71, 68,  2,\n",
      "        81, 78, 78, 69, 15]), tensor([53, 64, 74, 68,  2, 66, 64, 81, 68,  2, 78, 69,  2, 88, 78, 84, 81, 82,\n",
      "        68, 75, 69, 15,  2, 53, 71, 68,  2, 79, 81, 72, 66, 68,  2, 78, 69,  2,\n",
      "        74, 78, 75, 83, 78,  2, 83, 64, 77, 74, 82,  2, 71, 64, 82,  2, 73, 84,\n",
      "        76, 79, 68, 67,  2, 83, 71, 81, 78, 84, 70, 71,  2, 83, 71, 68,  2, 81,\n",
      "        78, 78, 69, 15,  0]))\n",
      "\n",
      "(tensor([ 1, 60, 39, 68, 68, 67,  2, 83, 71, 68,  2, 65, 68, 64, 82, 83,  2, 64,\n",
      "        77, 67,  2, 72, 83,  2, 86, 72, 75, 75,  2, 71, 68, 68, 67,  2, 88, 78,\n",
      "        84, 81,  2, 66, 64, 75, 75, 15,  2, 53, 64, 74, 68,  2, 85, 72, 79, 68,\n",
      "        81, 82,  2, 69, 81, 78, 76,  2, 83, 71, 68, 72, 81,  2, 75, 64, 72, 81,\n",
      "         2, 64, 77, 67,  2, 71, 64, 77, 70,  2, 83, 71, 68, 76,  2, 69, 81, 78,\n",
      "        76,  2, 64, 65, 78, 85, 68, 15,  2, 45, 68, 83,  2, 65, 75, 78, 78, 67,\n",
      "         2, 82, 66, 68, 77, 83,  2, 83, 71, 68,  2, 70, 81, 78, 84, 77, 67,  2,\n",
      "        78, 69,  2, 78, 84, 81,  2, 64, 77, 66, 68, 82, 83, 78, 81, 82, 15, 61,\n",
      "         0]), tensor([ 1, 60, 48, 65, 85, 72, 78, 84, 82, 75, 88,  2, 83, 71, 72, 82,  2, 86,\n",
      "        64, 82,  2, 78, 77, 66, 68,  2, 64,  2, 79, 75, 64, 66, 68,  2, 78, 69,\n",
      "         2, 70, 81, 68, 64, 83,  2, 81, 72, 83, 84, 64, 75,  2, 72, 76, 79, 78,\n",
      "        81, 83, 64, 77, 66, 68, 15, 61]), tensor([60, 48, 65, 85, 72, 78, 84, 82, 75, 88,  2, 83, 71, 72, 82,  2, 86, 64,\n",
      "        82,  2, 78, 77, 66, 68,  2, 64,  2, 79, 75, 64, 66, 68,  2, 78, 69,  2,\n",
      "        70, 81, 68, 64, 83,  2, 81, 72, 83, 84, 64, 75,  2, 72, 76, 79, 78, 81,\n",
      "        83, 64, 77, 66, 68, 15, 61,  0]))\n"
     ]
    }
   ],
   "source": [
    "dataset = CustomDataset(data)\n",
    "print(len(dataset))\n",
    "print()\n",
    "print(dataset.vocab)\n",
    "print()\n",
    "print(dataset[0])\n",
    "print()\n",
    "print(dataset[len(dataset)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f9c57d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data_points):\n",
    "    L_prevs = [len(p) for p,i,o in data_points]\n",
    "    L_currents  = [len(i) for p,i,o in data_points]\n",
    "    N_prevs = max(*L_prevs)\n",
    "    N_currents = max(*L_currents)\n",
    "    B = len(data_points)\n",
    "    prevs = torch.zeros((B,N_prevs),dtype=data_points[0][0].dtype)\n",
    "    ins = torch.zeros((B,N_currents),dtype=data_points[0][1].dtype)\n",
    "    outs = torch.zeros((B,N_currents),dtype=data_points[0][2].dtype)\n",
    "    for k in range(B):\n",
    "        l_prevs = L_prevs[k]\n",
    "        prevs[k,:l_prevs] = data_points[k][0]\n",
    "        l_currents = L_currents[k]\n",
    "        ins[k,:l_currents] = data_points[k][1]\n",
    "        outs[k,:l_currents] = data_points[k][2]\n",
    "    return((prevs,ins,outs),L_prevs,L_currents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac0c6fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset,\n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=True,\n",
    "                                         drop_last=True,\n",
    "                                         pin_memory=True,\n",
    "                                         collate_fn=collate_fn,\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49fcf15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainablePositionalEncoding(nn.Module):\n",
    "    def __init__(self,encoding_dim,num_of_features=None):\n",
    "        super(TrainablePositionalEncoding,self).__init__()\n",
    "        self.num_of_features = num_of_features\n",
    "        self.encoding_dim = encoding_dim\n",
    "        if encoding_dim%2!=0:\n",
    "            raise ValueError('encoding_dim should be a multiple of two!')\n",
    "        if num_of_features is None:\n",
    "            self.exp_linear = nn.Linear(1,encoding_dim//2,bias=True)\n",
    "            self.angle_linear = nn.Linear(1,encoding_dim//2,bias=False)\n",
    "        else:\n",
    "            self.exp_linear = nn.Linear(self.num_of_features,encoding_dim//2,bias=True)\n",
    "            self.angle_linear = nn.Linear(self.num_of_features,encoding_dim//2,bias=True)\n",
    "    def forward(self,x):\n",
    "        if self.num_of_features is None:\n",
    "            x = x.unsqueeze(-1)\n",
    "        exp_tensor = torch.exp(self.exp_linear(x)/80)\n",
    "        angle_tensor = self.angle_linear(x)\n",
    "        out = torch.cat((exp_tensor*torch.sin(angle_tensor),exp_tensor*torch.cos(angle_tensor)),dim=-1)\n",
    "        return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d78f2d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_positional_info(x,ch2i = dataset.ch2i):\n",
    "    try:\n",
    "        space_idx = add_positional_info.space_idx\n",
    "        punct_idx = add_positional_info.punct_idx\n",
    "    except AttributeError:\n",
    "        space_idx = ch2i[' ']\n",
    "        punct_idx = torch.tensor([ ch2i[s] for s in ['.','!','?'] ]).to(device)\n",
    "        add_positional_info.space_idx = space_idx\n",
    "        add_positional_info.punct_idx = punct_idx\n",
    "    punct_mask = torch.isin(x,punct_idx)\n",
    "    punct_mask = punct_mask.cumsum(axis=1)\n",
    "    space_mask = x==space_idx\n",
    "    out = 0\n",
    "    try:\n",
    "        punct_mask_max = punct_mask.max().item()\n",
    "    except RuntimeError:\n",
    "        punct_mask_max = 0\n",
    "    for punct_mark in range(punct_mask_max+1):\n",
    "        punct_mark_mask = (punct_mask==punct_mark)\n",
    "        out += (space_mask*punct_mark_mask).cumsum(axis=1)*punct_mark_mask\n",
    "    out = torch.stack((x,out),dim=2)\n",
    "    return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b1dc7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, out_dim, embedding_dim, rnn_units, n_layers=2):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.rnn_units = rnn_units\n",
    "        self.out_dim = out_dim\n",
    "        self.embedding = nn.Embedding(vocab_size,\n",
    "                                      embedding_dim,\n",
    "                                     )\n",
    "        self.pos_encoding = TrainablePositionalEncoding(embedding_dim)\n",
    "        self.grus = nn.ModuleList()\n",
    "        self.linears = nn.ModuleList()\n",
    "        self.initial_states = nn.ParameterList()\n",
    "        for submodel_layers in range(1,n_layers+1):\n",
    "            submodel_gru = nn.GRU(input_size=embedding_dim,\n",
    "                                          hidden_size=self.rnn_units,\n",
    "                                          num_layers=submodel_layers,\n",
    "                                          batch_first=True,\n",
    "                                         )\n",
    "            submodel_linear = nn.Linear(rnn_units,\n",
    "                                                out_dim,\n",
    "                                                bias=True,\n",
    "                                               )\n",
    "            self.grus.append(submodel_gru)\n",
    "            self.linears.append(submodel_linear)\n",
    "            self.initial_states.append(nn.Parameter(torch.randn((submodel_layers,self.rnn_units,))))\n",
    "        self.bias = nn.Parameter(torch.randn((out_dim,)))\n",
    "    def batch_initial_states(self,batch_size):\n",
    "        states = [ init_state.repeat((batch_size,1,1)).permute(1,0,2) for init_state in self.initial_states ]\n",
    "        return(states)\n",
    "    def forward(self, inputs, lengths, states=None,device=device):\n",
    "        batch_size = len(lengths)\n",
    "        if states is None:\n",
    "            states = self.batch_initial_states(batch_size)\n",
    "        x = self.embedding(inputs[...,0])\n",
    "        x += self.pos_encoding(inputs[...,1].float())\n",
    "        x = torch.nn.utils.rnn.pack_padded_sequence(x,lengths,batch_first=True,enforce_sorted=False)\n",
    "        out = 0\n",
    "        for k in range(len(self.grus)):\n",
    "            # Apply GRU\n",
    "            state = states[k]\n",
    "            submodel_gru = self.grus[k]\n",
    "            sub_out, state = submodel_gru(x,state)\n",
    "\n",
    "            # Apply linear transform\n",
    "            sub_out,_ = torch.nn.utils.rnn.pad_packed_sequence(sub_out, batch_first=True)\n",
    "            sub_out = sub_out[torch.arange(len(lengths)),torch.tensor(lengths).to(device)-1]\n",
    "            submodel_linear = self.linears[k]\n",
    "            sub_out = submodel_linear(sub_out)\n",
    "            \n",
    "            # Collect in output\n",
    "            out += sub_out\n",
    "        return(out)\n",
    "    def noisify(self,scale):\n",
    "        with torch.no_grad():\n",
    "            for p in self.grus.parameters():\n",
    "                p.add_(torch.randn_like(p),alpha=scale)\n",
    "            for p in self.linears.parameters():\n",
    "                p.add_(torch.randn_like(p),alpha=scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f871e452",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units, n_layers=2):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.rnn_units = rnn_units\n",
    "        self.embedding = nn.Embedding(vocab_size,\n",
    "                                      embedding_dim,\n",
    "                                     )\n",
    "        self.pos_encoding = TrainablePositionalEncoding(embedding_dim)\n",
    "        self.grus = nn.ModuleList()\n",
    "        self.linears = nn.ModuleList()\n",
    "        self.initial_states = nn.ParameterList()\n",
    "        for submodel_layers in range(1,n_layers+1):\n",
    "            submodel_gru = nn.GRU(input_size=embedding_dim,\n",
    "                                          hidden_size=self.rnn_units,\n",
    "                                          num_layers=submodel_layers,\n",
    "                                          batch_first=True,\n",
    "                                         )\n",
    "            submodel_linear = nn.Linear(rnn_units,\n",
    "                                                vocab_size,\n",
    "                                                bias=False,\n",
    "                                               )\n",
    "            self.grus.append(submodel_gru)\n",
    "            self.linears.append(submodel_linear)\n",
    "            self.initial_states.append(nn.Parameter(torch.randn((submodel_layers,self.rnn_units,))))\n",
    "        self.bias = nn.Parameter(torch.randn((vocab_size,)))\n",
    "    def batch_initial_states(self,batch_size):\n",
    "        states = [ init_state.repeat((batch_size,1,1)).permute(1,0,2) for init_state in self.initial_states ]\n",
    "        return(states)\n",
    "    def forward(self, inputs, encoding_tensor, lengths, states=None):\n",
    "        if states is None:\n",
    "            states = self.batch_initial_states(len(lengths))\n",
    "        batch_size = len(lengths)\n",
    "        x = self.embedding(inputs[...,0])\n",
    "        x += self.pos_encoding(inputs[...,1].float())\n",
    "        x += encoding_tensor[:,None,:]\n",
    "        x = torch.nn.utils.rnn.pack_padded_sequence(x,lengths,batch_first=True,enforce_sorted=False)\n",
    "        out = 0\n",
    "        for k in range(len(self.grus)):\n",
    "            # Apply GRU\n",
    "            state = states[k]\n",
    "            submodel_gru = self.grus[k]\n",
    "            sub_out, state = submodel_gru(x,state)\n",
    "            states[k] = state\n",
    "            \n",
    "            # Apply linear transform\n",
    "            sub_out,_ = torch.nn.utils.rnn.pad_packed_sequence(sub_out, batch_first=True)\n",
    "            submodel_linear = self.linears[k]\n",
    "            sub_out = submodel_linear(sub_out)\n",
    "            \n",
    "            # Collect in output\n",
    "            out += sub_out\n",
    "        \n",
    "        out += self.bias[None,None,:]\n",
    "        return(out,states)\n",
    "    def noisify(self,scale):\n",
    "        with torch.no_grad():\n",
    "            for p in self.grus.parameters():\n",
    "                p.add_(torch.randn_like(p),alpha=scale)\n",
    "            for p in self.linears.parameters():\n",
    "                p.add_(torch.randn_like(p),alpha=scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75a34c5d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder(\n",
      "  (embedding): Embedding(90, 256)\n",
      "  (pos_encoding): TrainablePositionalEncoding(\n",
      "    (exp_linear): Linear(in_features=1, out_features=128, bias=True)\n",
      "    (angle_linear): Linear(in_features=1, out_features=128, bias=False)\n",
      "  )\n",
      "  (grus): ModuleList(\n",
      "    (0): GRU(256, 256, batch_first=True)\n",
      "    (1): GRU(256, 256, num_layers=2, batch_first=True)\n",
      "    (2): GRU(256, 256, num_layers=3, batch_first=True)\n",
      "    (3): GRU(256, 256, num_layers=4, batch_first=True)\n",
      "    (4): GRU(256, 256, num_layers=5, batch_first=True)\n",
      "  )\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (3): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (4): Linear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (initial_states): ParameterList(\n",
      "      (0): Parameter containing: [torch.FloatTensor of size 1x256]\n",
      "      (1): Parameter containing: [torch.FloatTensor of size 2x256]\n",
      "      (2): Parameter containing: [torch.FloatTensor of size 3x256]\n",
      "      (3): Parameter containing: [torch.FloatTensor of size 4x256]\n",
      "      (4): Parameter containing: [torch.FloatTensor of size 5x256]\n",
      "  )\n",
      ")\n",
      "Decoder(\n",
      "  (embedding): Embedding(90, 256)\n",
      "  (pos_encoding): TrainablePositionalEncoding(\n",
      "    (exp_linear): Linear(in_features=1, out_features=128, bias=True)\n",
      "    (angle_linear): Linear(in_features=1, out_features=128, bias=False)\n",
      "  )\n",
      "  (grus): ModuleList(\n",
      "    (0): GRU(256, 256, batch_first=True)\n",
      "    (1): GRU(256, 256, num_layers=2, batch_first=True)\n",
      "    (2): GRU(256, 256, num_layers=3, batch_first=True)\n",
      "    (3): GRU(256, 256, num_layers=4, batch_first=True)\n",
      "    (4): GRU(256, 256, num_layers=5, batch_first=True)\n",
      "  )\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=256, out_features=90, bias=False)\n",
      "    (1): Linear(in_features=256, out_features=90, bias=False)\n",
      "    (2): Linear(in_features=256, out_features=90, bias=False)\n",
      "    (3): Linear(in_features=256, out_features=90, bias=False)\n",
      "    (4): Linear(in_features=256, out_features=90, bias=False)\n",
      "  )\n",
      "  (initial_states): ParameterList(\n",
      "      (0): Parameter containing: [torch.FloatTensor of size 1x256]\n",
      "      (1): Parameter containing: [torch.FloatTensor of size 2x256]\n",
      "      (2): Parameter containing: [torch.FloatTensor of size 3x256]\n",
      "      (3): Parameter containing: [torch.FloatTensor of size 4x256]\n",
      "      (4): Parameter containing: [torch.FloatTensor of size 5x256]\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(dataset.vocab)\n",
    "embedding_dim = 256\n",
    "rnn_units = 256\n",
    "n_layers = 5\n",
    "\n",
    "encoder = Encoder(vocab_size=vocab_size,\n",
    "                  out_dim = embedding_dim,\n",
    "                  embedding_dim = embedding_dim,\n",
    "                  rnn_units=rnn_units,\n",
    "                  n_layers = n_layers,\n",
    "             )\n",
    "encoder.to(device)\n",
    "print(encoder)\n",
    "\n",
    "decoder = Decoder(vocab_size=vocab_size,\n",
    "                  embedding_dim = embedding_dim,\n",
    "                  rnn_units=rnn_units,\n",
    "                  n_layers = n_layers,\n",
    "             )\n",
    "decoder.to(device)\n",
    "print(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79403269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder parameters: 6277760\n",
      "decoder parameters: 6063834\n"
     ]
    }
   ],
   "source": [
    "print('encoder parameters:',sum( np.prod(p.shape) for p in encoder.parameters()))\n",
    "print('decoder parameters:',sum( np.prod(p.shape) for p in decoder.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f4707f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded encoder\n",
      "Loaded decoder\n"
     ]
    }
   ],
   "source": [
    "encoder.load_state_dict(torch.load(encoder_name))\n",
    "encoder.to(device)\n",
    "print('Loaded encoder')\n",
    "decoder.load_state_dict(torch.load(decoder_name))\n",
    "decoder.to(device)\n",
    "print('Loaded decoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e3024d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 2])\n",
      "tensor([[[1, 0],\n",
      "         [0, 0]]])\n",
      "\r\n",
      "\n",
      "\n",
      "torch.Size([1, 77, 2])\n",
      "tensor([[[ 1,  0],\n",
      "         [53,  0],\n",
      "         [64,  0],\n",
      "         [74,  0],\n",
      "         [68,  0],\n",
      "         [ 2,  1],\n",
      "         [66,  1],\n",
      "         [64,  1],\n",
      "         [81,  1],\n",
      "         [68,  1],\n",
      "         [ 2,  2],\n",
      "         [78,  2],\n",
      "         [69,  2],\n",
      "         [ 2,  3],\n",
      "         [88,  3],\n",
      "         [78,  3],\n",
      "         [84,  3],\n",
      "         [81,  3],\n",
      "         [82,  3],\n",
      "         [68,  3],\n",
      "         [75,  3],\n",
      "         [69,  3],\n",
      "         [15,  0],\n",
      "         [ 2,  1],\n",
      "         [53,  1],\n",
      "         [71,  1],\n",
      "         [68,  1],\n",
      "         [ 2,  2],\n",
      "         [79,  2],\n",
      "         [81,  2],\n",
      "         [72,  2],\n",
      "         [66,  2],\n",
      "         [68,  2],\n",
      "         [ 2,  3],\n",
      "         [78,  3],\n",
      "         [69,  3],\n",
      "         [ 2,  4],\n",
      "         [74,  4],\n",
      "         [78,  4],\n",
      "         [75,  4],\n",
      "         [83,  4],\n",
      "         [78,  4],\n",
      "         [ 2,  5],\n",
      "         [83,  5],\n",
      "         [64,  5],\n",
      "         [77,  5],\n",
      "         [74,  5],\n",
      "         [82,  5],\n",
      "         [ 2,  6],\n",
      "         [71,  6],\n",
      "         [64,  6],\n",
      "         [82,  6],\n",
      "         [ 2,  7],\n",
      "         [73,  7],\n",
      "         [84,  7],\n",
      "         [76,  7],\n",
      "         [79,  7],\n",
      "         [68,  7],\n",
      "         [67,  7],\n",
      "         [ 2,  8],\n",
      "         [83,  8],\n",
      "         [71,  8],\n",
      "         [81,  8],\n",
      "         [78,  8],\n",
      "         [84,  8],\n",
      "         [70,  8],\n",
      "         [71,  8],\n",
      "         [ 2,  9],\n",
      "         [83,  9],\n",
      "         [71,  9],\n",
      "         [68,  9],\n",
      "         [ 2, 10],\n",
      "         [81, 10],\n",
      "         [78, 10],\n",
      "         [78, 10],\n",
      "         [69, 10],\n",
      "         [15,  0]]])\n",
      "\r",
      "Take care of yourself. The price of kolto tanks has jumped through the roof.\n",
      "00011111222333333333011112222223334444445555556666777777788888888999910101010100\n",
      "torch.Size([1, 256])\n",
      "tensor([[ 9.4981e-01, -4.1792e-01, -8.3136e-02, -1.0611e+00, -3.2179e-01,\n",
      "          2.5078e-01,  1.8728e-02, -2.3663e-01,  6.1133e-01, -4.4412e-01,\n",
      "         -2.8499e-01,  1.8188e-01,  5.4763e-02,  7.3703e-01, -2.0342e-01,\n",
      "         -3.3186e-01, -9.7295e-02, -6.8658e-02, -5.4997e-02,  4.9499e-01,\n",
      "         -1.1435e-01, -6.4453e-01, -7.8575e-02,  3.0996e-01, -7.5081e-02,\n",
      "          1.9199e+00, -6.6897e-01, -2.6385e-02,  2.1466e-01,  3.1010e-02,\n",
      "          1.0827e+00,  1.3079e-01,  2.5515e-01, -9.1852e-01,  1.7214e-01,\n",
      "         -1.0487e-01,  3.7881e-01, -1.2711e+00, -1.0442e-01,  5.1768e-01,\n",
      "          6.3982e-01,  1.3210e-01,  2.5518e-01,  5.6329e-01, -5.7698e-01,\n",
      "          9.1019e-01,  7.8387e-01,  2.5371e-01, -4.4181e-01, -3.1386e-01,\n",
      "         -1.1516e+00, -6.0138e-01, -3.4056e-01, -1.5831e-01,  2.2291e-01,\n",
      "         -6.6732e-01,  9.4725e-03, -2.2210e-01, -5.9218e-02, -4.3775e-01,\n",
      "         -3.8151e-02,  1.0820e+00, -7.3662e-01, -3.5018e-02,  6.9606e-01,\n",
      "          4.8300e-01,  4.9962e-01,  1.5052e-01,  6.0499e-01,  2.9229e-01,\n",
      "         -3.0050e-01, -3.4513e-01,  5.6803e-01, -2.6195e-01,  5.2329e-01,\n",
      "          7.9056e-01,  2.8931e-01, -8.1514e-01,  1.9087e-01, -1.5152e-01,\n",
      "         -7.5443e-01, -1.2215e-01, -1.7228e-02, -4.5406e-01, -2.7810e-01,\n",
      "          2.2650e-01, -2.5496e-01,  6.9619e-01, -8.1640e-01, -1.5692e-01,\n",
      "         -3.5572e-01, -7.6859e-01, -3.3158e-01,  5.3689e-01,  2.0783e-02,\n",
      "         -3.8215e-01,  4.2725e-02,  1.5306e+00, -4.0341e-01, -8.4272e-01,\n",
      "          1.7084e-01,  4.7010e-01,  2.5379e-01,  3.6076e-01,  2.3123e-01,\n",
      "         -1.1134e-01,  1.0122e+00, -1.8646e-01,  1.6780e+00,  1.1064e-01,\n",
      "          3.8881e-01,  5.1195e-01,  1.8820e-01,  9.0353e-01, -1.8722e-01,\n",
      "         -3.8905e-01,  2.7546e-01,  1.2280e-01, -5.5980e-01,  3.1669e-01,\n",
      "          4.4117e-01, -1.1488e-01,  4.7750e-01,  1.7315e-01,  8.3122e-01,\n",
      "          1.2320e-01,  5.3442e-01,  3.4279e-02, -1.8642e-01, -8.3271e-01,\n",
      "         -4.7939e-01, -4.6624e-01,  3.4956e-01, -1.6902e-01,  2.0739e-01,\n",
      "          1.0336e+00, -5.5987e-01,  1.1014e-01,  1.3116e-02,  6.8908e-01,\n",
      "         -6.7798e-01, -1.3538e-01, -8.2211e-01, -8.3923e-01, -2.7190e-02,\n",
      "         -3.8304e-01,  5.0460e-01, -3.8425e-01, -5.4426e-01, -7.2023e-02,\n",
      "         -7.0000e-01, -6.7787e-01, -4.1527e-01,  1.5585e-01, -6.4767e-01,\n",
      "         -1.1160e+00, -5.3336e-01, -1.0391e+00,  5.6694e-04, -7.9012e-02,\n",
      "         -4.1195e-01, -3.2441e-01, -1.1662e-01,  8.6397e-02, -6.9691e-01,\n",
      "         -1.3095e+00,  4.2703e-01,  7.2293e-01, -3.7867e-01, -5.6132e-01,\n",
      "          5.7491e-01, -7.6809e-01, -5.8139e-01, -1.3867e-01,  5.3540e-01,\n",
      "          1.2644e-01, -1.9830e-01,  5.0595e-01, -3.2448e-01,  5.8374e-01,\n",
      "          5.6824e-01, -4.7370e-01, -2.1269e-01, -2.7381e-01, -3.7227e-01,\n",
      "          5.4425e-03, -6.8068e-01,  4.3200e-01, -2.0556e-01, -2.1485e-01,\n",
      "         -5.1242e-01, -3.1722e-01,  1.5377e-01, -6.1988e-01, -1.0426e+00,\n",
      "         -5.8867e-01, -8.2703e-01, -2.8003e-01, -5.0470e-01,  2.8730e-02,\n",
      "         -2.2529e-01, -1.4298e-01, -4.7741e-01, -4.3150e-01,  2.5316e-01,\n",
      "          9.5539e-01, -4.0315e-01,  2.4302e-01, -2.6790e-01,  6.1723e-01,\n",
      "          1.4866e-01,  3.2032e-01,  3.0231e-01, -1.1177e-01, -1.3743e+00,\n",
      "          4.9563e-01, -2.0924e-01,  3.7822e-01, -2.2004e+00,  2.6060e-01,\n",
      "          3.8386e-02, -1.5529e-01, -3.5759e-01, -7.6730e-01, -3.7742e-01,\n",
      "         -4.1973e-01,  2.9177e-01, -1.4366e-01, -2.0846e-01, -5.6900e-01,\n",
      "         -4.4445e-01,  3.4562e-01, -3.2682e-01, -3.2412e-01,  7.1852e-01,\n",
      "         -9.3800e-01, -7.0154e-01,  6.3718e-01, -2.3722e-01, -6.8335e-01,\n",
      "         -2.0981e-01,  2.8269e-02,  8.1338e-02, -2.9406e-01, -3.9935e-01,\n",
      "         -6.6596e-01,  3.3599e-01,  4.6837e-01,  2.4232e-01,  5.3943e-01,\n",
      "          4.7826e-02, -4.4980e-01, -2.0554e-01, -3.0849e-01,  2.0005e-01,\n",
      "         -1.0158e-01]], grad_fn=<AddBackward0>)\n",
      "[tensor([[[ 0.7780, -0.5456,  0.8678,  0.9669, -0.8273, -0.1242, -0.9991,\n",
      "          -0.6401,  0.9866, -0.4059,  0.7794,  0.9990, -0.9705, -0.5833,\n",
      "          -0.8677, -0.9974,  0.9954,  0.5503,  0.5953,  0.9568, -0.9979,\n",
      "           0.2115,  0.9623,  0.9939,  0.7640, -0.9933, -0.9976,  0.5245,\n",
      "           0.1108, -0.9671,  0.9583,  0.8772,  0.9747,  0.9165,  0.9973,\n",
      "          -0.9892, -0.9716, -0.7985,  0.8936,  0.9599,  0.9952, -0.9476,\n",
      "          -0.6125, -0.9183, -0.9881, -0.3224,  0.6273, -0.4782,  0.9961,\n",
      "           0.9971, -0.9982,  0.9873,  0.9697,  0.9779, -0.7884,  0.2188,\n",
      "          -0.9958, -0.9656, -0.9892, -0.4459, -0.9965, -0.8789,  0.9899,\n",
      "          -0.7788, -0.9247, -0.5167, -0.9999,  0.9995,  1.0000,  0.9889,\n",
      "          -0.9981,  0.6689,  0.9704,  0.9052, -0.9901, -0.9120, -0.4331,\n",
      "          -0.9508, -0.9592, -0.4049,  0.9710,  0.9545,  0.8742, -0.8941,\n",
      "          -0.9992, -0.9999,  0.9955, -0.9964,  0.9766, -0.0024,  0.9838,\n",
      "           0.9791,  0.8547,  0.8997, -0.7037, -0.9832, -0.9886,  0.9719,\n",
      "           0.5454, -0.7812, -0.6307,  0.9296,  0.9985, -0.5102, -0.9333,\n",
      "           0.9600, -0.9981,  0.2550, -0.9086, -0.9892, -0.9226,  0.8034,\n",
      "           0.8262,  0.9205, -0.9994, -0.9898, -0.9881,  0.9507,  0.8679,\n",
      "          -0.9385, -0.9952, -0.4907,  0.9968,  0.9015,  0.9987,  0.4018,\n",
      "           0.9183, -0.9942,  0.8064, -0.9956,  0.8943,  0.3291, -0.9999,\n",
      "           0.9991, -0.8739, -0.9822, -0.9881, -0.8554,  0.9999, -0.9923,\n",
      "          -0.2671, -0.3273,  0.9983, -0.9614, -0.6533,  0.9983, -0.8894,\n",
      "           0.9862,  0.7904,  0.2383, -0.9913,  0.8985, -0.1308, -0.9388,\n",
      "           0.8235,  0.6205, -0.8826,  0.9929,  0.4873,  0.9690, -0.4865,\n",
      "          -0.9112, -0.8675,  0.7275, -0.9970,  0.9426, -0.8391,  0.9886,\n",
      "          -0.9969,  0.9326,  1.0000,  0.1840,  0.2566,  0.9952,  0.9934,\n",
      "          -0.9729,  1.0000,  0.9998, -0.9959,  0.8708,  0.9756,  0.9826,\n",
      "           0.9616,  0.9411,  0.9989, -0.9400, -0.9839, -0.8341,  0.9968,\n",
      "          -0.5389, -0.9872,  0.9880,  0.4060, -0.9712, -0.9956, -0.4936,\n",
      "           0.9921, -0.4491, -1.0000,  0.9999, -1.0000,  0.7451,  0.9994,\n",
      "          -0.8077, -0.0879,  0.4739,  0.1659, -0.2405,  0.4437, -0.9982,\n",
      "          -0.6972, -0.9972, -0.9999, -0.0938, -0.0840, -0.9242, -0.1735,\n",
      "           0.9349, -0.8674, -0.4941,  0.8023, -0.9995, -0.7299, -0.9302,\n",
      "          -0.9995, -0.9997, -0.8308,  0.9940, -0.7521,  0.9660, -0.7557,\n",
      "          -0.8757, -0.9982, -0.8850,  0.9839,  0.9645,  0.9411,  0.4041,\n",
      "          -0.4366,  0.9647, -0.9698,  0.2376,  0.7869,  0.3296, -0.9970,\n",
      "           0.5311, -1.0000,  0.9982,  0.9999,  0.3669, -0.9259, -0.9330,\n",
      "           0.9920, -0.0334, -0.9800, -0.9718]]],\n",
      "       grad_fn=<IndexSelectBackward0>), tensor([[[-0.0466, -1.0000, -0.7795,  0.9777, -0.8262,  0.9907,  0.6041,\n",
      "          -0.2129,  0.4580,  0.9147, -0.5048, -0.7287,  0.9338, -0.9957,\n",
      "          -0.9590, -0.9837, -0.7776,  0.7184,  0.9836, -0.9998,  0.2997,\n",
      "          -0.9799,  0.4208, -0.5519,  0.9749, -0.9987, -0.9996, -0.3696,\n",
      "           0.7775,  0.8848,  0.8062,  0.3609, -0.8348, -0.0552,  0.9890,\n",
      "          -0.8355, -0.9303,  0.9221, -0.9421, -0.8868, -0.7748, -0.9748,\n",
      "           0.5934, -0.9998,  0.9797, -0.2668,  0.4050,  0.8469,  0.9384,\n",
      "           0.7338,  0.9493,  0.9982, -0.2778,  0.9897, -0.5828, -0.9953,\n",
      "          -0.9992,  0.6668,  0.9732,  0.9883, -1.0000, -0.6948, -0.9221,\n",
      "           0.9312,  0.9984,  0.9857,  0.5638,  0.9415, -0.8149,  0.8268,\n",
      "           0.1555,  0.9854,  0.9139,  0.9773, -0.9381, -0.4339,  0.8323,\n",
      "           0.9268, -0.6831,  0.9430,  0.4133, -0.1871,  0.2114, -0.7503,\n",
      "           0.5847, -0.9755,  0.9399, -0.9362,  0.1516,  0.3198,  0.3760,\n",
      "          -0.9860,  0.9434,  0.8600,  0.8509,  0.4458, -1.0000,  0.9942,\n",
      "          -0.9895, -0.9050, -1.0000,  0.1331,  0.8345,  0.9930,  0.8692,\n",
      "          -0.4183, -0.6170, -0.9813,  0.8560, -0.3322,  0.9914, -0.5738,\n",
      "          -0.9520,  0.1951,  0.9699,  0.6619,  0.9649,  0.2121, -0.8395,\n",
      "           0.8990,  0.9875,  0.1298, -0.9075, -0.9983, -0.9052, -0.4942,\n",
      "          -0.5735,  0.9850, -0.0722,  0.9330,  0.8615, -0.6709,  0.9970,\n",
      "           0.9853,  0.9884,  0.7611, -0.9909, -0.9982,  0.9999, -0.8671,\n",
      "          -1.0000, -0.9471, -0.9621, -0.4192, -0.8660,  0.9915,  0.4082,\n",
      "           0.6681, -0.9733,  0.9374,  0.1952,  0.7169, -0.9965,  1.0000,\n",
      "           0.9974,  0.9796, -0.7368,  0.9525,  0.8491, -0.9392,  0.9984,\n",
      "          -0.9330, -0.1059,  0.9780,  0.8910,  0.9872, -0.4606,  0.9397,\n",
      "          -0.9983,  0.8207,  0.0676, -0.9994,  0.8672,  0.9237,  0.9539,\n",
      "           0.7031,  0.9885, -0.9857, -0.9334,  0.2397,  0.1848, -0.9986,\n",
      "           0.9786,  0.1913, -0.1196, -0.6629,  0.8958, -1.0000,  0.8197,\n",
      "           0.5398, -0.2761, -0.9835,  1.0000,  0.9988,  0.1496, -0.9178,\n",
      "          -0.0961, -0.9513, -0.4580,  0.2506, -0.8452, -0.8663, -0.6957,\n",
      "          -0.9963, -0.9999, -0.9855,  0.9700, -0.5930,  0.9959, -0.9991,\n",
      "           0.9645,  0.9611,  0.2504, -0.9571, -0.7574,  0.8447, -0.9814,\n",
      "          -0.5039, -0.9964, -0.9904, -0.9623,  0.8963,  0.1269,  0.9780,\n",
      "          -0.7419,  0.5433, -0.9766,  0.8242,  0.6996, -0.8835, -0.5223,\n",
      "          -0.7767,  0.9472,  0.2837, -0.1045,  0.6138, -0.9252, -0.9076,\n",
      "          -0.5237, -0.1442, -0.3321,  0.9956, -0.5114, -0.8514, -0.6299,\n",
      "           0.9940,  0.9657,  0.5650,  0.9862, -0.5442,  0.4172, -0.9735,\n",
      "          -0.9370,  0.9868, -0.9839, -0.9333]],\n",
      "\n",
      "        [[ 0.9802, -0.9903,  0.8511, -0.0674,  0.9476, -0.8220,  0.8970,\n",
      "          -0.9593,  0.8507,  0.3228, -0.0522,  0.9928, -0.5672, -0.9599,\n",
      "           0.9212, -0.7624,  0.2048, -0.1680,  0.7481, -0.9717,  0.9535,\n",
      "          -0.6926,  0.9938,  0.8515,  0.7050,  0.4357, -0.1847,  0.9496,\n",
      "          -0.5683, -0.3860,  0.9655,  0.9894, -0.9855, -0.9994, -0.9718,\n",
      "           0.6888, -0.8800, -0.8640, -0.9984,  0.1582,  0.9555,  0.1935,\n",
      "           0.4556, -0.9971, -0.9614, -0.9241,  0.6198, -0.8552, -0.2247,\n",
      "           0.9982, -0.7126,  0.0555,  0.9877, -0.8526,  0.9222,  0.3969,\n",
      "          -0.5513, -0.8174,  0.9571, -0.9891, -0.9236,  0.9970,  0.9786,\n",
      "          -0.8662,  0.9563, -0.9820, -0.8705, -0.7490, -0.9869,  0.6685,\n",
      "          -0.9994, -0.9907, -0.9983,  0.9000, -0.9733, -0.8109,  0.9895,\n",
      "           0.2579, -0.7292, -0.7999,  0.1314, -0.9349,  0.9073, -0.6589,\n",
      "          -0.9212, -0.0344,  0.0557,  0.9581,  0.9864,  0.8763, -0.7921,\n",
      "           0.6481,  0.9069, -0.1852,  0.1739,  0.3917,  0.9775, -0.9065,\n",
      "           0.9997,  0.9907,  0.9950,  0.9384,  0.8056,  0.8405, -0.9488,\n",
      "           0.2150,  0.9243, -0.8149,  0.9597,  0.9960, -0.9738, -0.1258,\n",
      "           0.7805,  0.8334, -0.5921, -0.7224, -0.6031,  0.5349,  0.8014,\n",
      "           0.9174,  0.2709,  0.9684,  0.3351, -0.9949,  0.2100, -0.7190,\n",
      "           0.4251,  0.9208, -0.9468, -0.8272, -0.4023,  0.9435, -0.9963,\n",
      "           0.9123, -0.5671,  0.9721, -0.9993,  0.7846,  0.8677,  0.3525,\n",
      "          -0.9991,  0.9895, -0.8426, -0.9956,  0.5455,  0.9272, -0.9966,\n",
      "          -0.9576,  0.3369,  0.9805,  0.8734, -0.9500,  0.9998,  0.4545,\n",
      "          -0.2394, -0.9841, -0.5168,  0.6556, -0.9878,  0.8753, -0.9646,\n",
      "           0.6662,  0.4962,  0.8179,  0.7816, -0.9097,  0.8110, -0.2440,\n",
      "          -0.8859,  0.9995, -0.9665,  0.9342, -0.0119, -0.6767, -0.2510,\n",
      "          -0.5614, -0.6578, -0.8209,  0.9782, -0.8916,  0.9918,  0.9549,\n",
      "          -0.8522, -0.9464, -0.1529, -0.9992,  0.1079,  0.9868, -0.9886,\n",
      "          -0.9340,  0.0252, -0.9126,  0.7816, -0.6451,  0.9588, -0.9282,\n",
      "           0.7054,  0.1598,  0.3612,  0.8612,  0.9869, -0.8526,  0.9719,\n",
      "          -0.1865,  0.9286, -0.9965,  0.9435, -0.9846,  0.9895,  0.9818,\n",
      "           0.8357,  0.1743, -0.4834, -0.7655, -0.9922,  0.8340, -0.0258,\n",
      "          -0.5741, -0.8423, -0.9672,  0.6365,  0.5168, -0.5098,  0.3442,\n",
      "           0.9930, -0.8920,  0.4347,  0.9833,  0.9828, -0.6625,  0.9952,\n",
      "          -0.1377,  0.5778,  0.2125,  0.9764,  0.6632,  0.9984, -0.9981,\n",
      "           0.1143,  0.1066, -0.8159, -0.0616, -0.8985,  0.4989, -0.9890,\n",
      "          -0.8894, -0.8466, -0.9386,  0.8905,  0.8943, -0.6475, -0.9904,\n",
      "          -0.1082,  0.5220, -0.8517, -0.9366]]],\n",
      "       grad_fn=<IndexSelectBackward0>), tensor([[[-0.9312, -0.9189, -0.8286, -0.1395, -0.9660,  0.5100,  0.9515,\n",
      "          -0.2242, -0.9696, -0.0421, -0.1737, -0.6579, -0.9867, -0.8233,\n",
      "           0.1922,  0.6050,  0.8941, -0.2845,  0.8312,  0.9889,  0.1237,\n",
      "          -0.9901,  0.9998, -0.9760,  0.2730, -0.9907, -0.3065,  0.2246,\n",
      "           0.9123,  0.7755,  0.8988, -0.9071, -0.5829,  0.3792,  0.5514,\n",
      "           0.7874, -0.2204, -0.4450,  0.1334,  0.2678,  0.2015,  0.9897,\n",
      "          -0.7600,  0.9217,  0.9797, -0.6439,  0.8273,  0.9818,  0.6720,\n",
      "           0.8394, -0.9484,  0.6385,  0.9170,  0.6230, -0.6408, -0.9973,\n",
      "           0.8993,  0.6342, -0.9921,  0.8238, -0.5355, -0.7537, -0.9350,\n",
      "          -0.7923,  0.8552,  0.5369,  0.7802, -0.9221, -0.7312,  0.0637,\n",
      "           0.1752, -0.9966, -0.9289, -0.9824,  0.3209,  0.9919, -0.5229,\n",
      "           0.9698, -0.9602, -0.9925, -0.8841,  0.5818, -0.6932, -0.9779,\n",
      "           0.9956,  0.9989,  0.9854,  0.6819,  0.6778, -0.2502,  0.4323,\n",
      "           0.5057,  0.9937,  0.5533, -0.9909, -0.0117, -0.9243, -0.8961,\n",
      "          -0.8975, -0.1016,  0.9313,  0.2168, -0.4721,  0.9999, -0.3093,\n",
      "          -0.0658,  0.7854, -0.9648, -0.5518,  0.7146,  0.9899,  0.9907,\n",
      "           0.7149,  0.2784,  0.9931, -0.8357,  0.9743,  0.9943,  0.2175,\n",
      "          -0.9921, -0.8991, -0.6883,  0.6134, -0.8844,  0.9215,  0.9930,\n",
      "          -0.3656,  0.9977,  0.9453, -0.6989, -0.4507,  0.9169,  0.8640,\n",
      "          -0.7176,  0.8406, -0.9823, -1.0000,  0.9051,  0.9961, -0.3509,\n",
      "          -0.7467, -0.9999,  0.9000,  0.9989, -0.0274,  0.1507, -0.9622,\n",
      "          -0.8608,  0.7604, -0.9880, -0.9889,  0.9990, -0.8048,  0.1234,\n",
      "          -0.7852, -0.9286, -0.9985, -0.7826, -0.9711,  0.4031,  0.9550,\n",
      "          -0.9983, -0.2353,  0.1918, -0.9047,  0.9076, -0.7136, -0.0991,\n",
      "          -0.9908,  0.4253, -0.6438, -0.5651, -0.9831,  0.9806,  0.9838,\n",
      "           0.9318,  0.9951,  0.1053,  0.9980, -0.9978, -0.7581,  0.9995,\n",
      "          -0.9862, -0.9757, -0.7655, -0.3284,  0.1062, -0.9896, -0.1523,\n",
      "           0.9501, -0.8717, -0.8438, -0.7204, -0.8603, -0.7016,  0.8147,\n",
      "          -0.5614, -0.9971,  0.9865, -0.9603,  0.4416,  0.5299,  0.9977,\n",
      "           0.9119, -0.8273, -0.9873, -0.6426, -0.7564, -0.7342, -0.9964,\n",
      "          -0.9922,  0.3810,  0.9979, -0.8781, -0.1589,  0.7048,  0.3037,\n",
      "          -0.7930,  0.6106,  0.3826,  0.7514,  0.9883,  0.9551, -0.9653,\n",
      "           0.3142, -0.9923,  0.8720,  0.8505,  0.9655,  0.6937,  0.9810,\n",
      "           0.1595, -0.9357, -0.8716,  0.9787, -0.9597,  0.1913,  0.9519,\n",
      "          -0.9972,  0.5700, -0.9622,  0.9927,  0.9727, -0.9959,  0.8949,\n",
      "          -0.9249,  0.0808,  0.2264, -0.9902, -0.8872,  0.9818, -0.8950,\n",
      "          -0.9385,  0.9980,  0.1858,  0.9233]],\n",
      "\n",
      "        [[ 0.6663,  0.7499,  0.3129,  0.7845, -0.9766,  0.5438,  0.1298,\n",
      "          -0.0459,  0.8402, -0.7899, -0.1824, -0.5294,  0.8311, -0.9979,\n",
      "          -0.9111, -0.6466, -0.0356,  0.8534,  0.4674,  0.7741,  0.9281,\n",
      "           0.9447, -0.9412,  0.9301,  0.0822,  0.7843, -0.3789, -0.5716,\n",
      "           0.3659, -0.7743, -0.5859,  0.8718, -0.9538,  0.6723, -0.9982,\n",
      "           0.8960, -0.8667,  0.9489, -0.9907, -0.7305, -0.9384,  0.9535,\n",
      "          -0.9609, -0.9867,  0.9614,  0.4966, -0.2697,  0.8389,  0.7422,\n",
      "          -0.2431, -0.2485, -0.9048, -0.9998, -0.3964,  0.9643, -0.9707,\n",
      "          -0.0675,  0.0518, -0.9840, -0.6478,  0.3912,  0.8066,  0.9792,\n",
      "          -0.8873,  0.9080,  0.9635, -0.9297,  0.3266, -0.0093, -0.9836,\n",
      "           0.9153,  0.6369, -0.5121,  0.9913, -0.9472,  0.7670, -0.7146,\n",
      "           0.9810, -0.4489, -0.9908, -0.3505, -0.8760, -0.5195,  0.8799,\n",
      "          -0.9055,  0.8063, -0.3332, -0.9265,  0.9884, -0.3229, -0.8676,\n",
      "           0.7908, -0.1520, -0.0843, -0.1794, -0.9709,  0.8347,  0.9934,\n",
      "          -0.8629, -0.3958,  0.9797, -0.4007,  0.3359, -0.9527,  0.7693,\n",
      "           0.0808,  0.1174, -0.9481, -0.9878,  0.9984, -0.4190,  0.2876,\n",
      "           0.0809, -0.9939,  0.6341, -0.9953, -0.9874, -0.5877, -0.8458,\n",
      "          -0.9995,  0.3333,  0.9995,  0.5645,  0.9882,  0.9871,  0.7904,\n",
      "           0.2003, -0.9794, -0.1783,  0.7746,  0.9729,  0.9870, -0.8549,\n",
      "           0.0750,  0.7785, -0.8984, -0.8926, -0.7146, -0.8468,  0.9980,\n",
      "          -0.9546, -0.2463,  0.9251,  0.9708, -0.5783,  0.5812, -0.8416,\n",
      "           0.2138,  0.9892,  0.8880, -0.9007,  0.7550,  0.2412, -0.7963,\n",
      "           0.0522, -0.8224, -0.9928,  0.9185,  0.4035,  0.8303,  0.8741,\n",
      "          -0.8542, -0.1288,  0.9926,  0.6252, -0.6991,  0.8773, -0.9605,\n",
      "           0.7543, -0.2032,  0.8071, -0.9196,  0.2036,  0.0673,  0.9426,\n",
      "          -0.5632,  0.9905,  0.6846,  0.9565,  0.9563, -0.0321, -0.0300,\n",
      "           0.2796,  0.9775, -0.9291, -0.8539, -0.9157,  0.9760,  0.9647,\n",
      "           0.9923, -0.8227,  0.8765, -0.5247, -0.7039, -0.7928, -0.0326,\n",
      "           0.9999,  0.2574,  0.8444,  0.8458,  0.5539, -0.1363, -0.9964,\n",
      "          -0.1860, -0.8241,  0.9055, -0.6709, -0.9671,  0.9985,  0.9996,\n",
      "           0.0389, -0.8211, -0.1609,  0.9840, -0.0055, -0.3681, -0.9349,\n",
      "          -0.1474, -0.1031,  0.7900, -0.8264,  0.7171, -0.8881, -0.6766,\n",
      "          -0.9398,  0.8284, -0.9249, -0.8605,  0.9792,  0.2038, -0.7309,\n",
      "          -0.2379,  0.3934,  0.5694, -0.4865,  0.9403,  0.3296,  0.8535,\n",
      "          -0.8614,  0.1175, -0.7712, -0.9984,  0.8842, -0.9937, -0.9071,\n",
      "           0.7294, -0.1533, -0.9750,  0.7791,  0.9996, -0.9858, -0.3960,\n",
      "           0.8203,  0.1184, -0.9596,  0.5019]],\n",
      "\n",
      "        [[-0.9954, -0.5725, -0.9909, -0.5439,  0.7740, -0.8096,  0.5116,\n",
      "           0.9061, -0.8196,  0.6092, -0.3432,  0.2104, -0.0058,  0.1554,\n",
      "          -0.9795,  0.9990,  0.6573, -0.9920,  0.9979,  0.9283, -0.4220,\n",
      "          -0.9935,  0.3784,  0.9367,  0.6258,  0.9702, -0.5705, -0.7431,\n",
      "           0.5354,  1.0000,  0.9150, -0.4296, -0.7487,  0.9936,  0.9492,\n",
      "           1.0000,  0.3453, -0.9871, -0.9317, -0.9462,  0.9056,  0.9825,\n",
      "           0.8830, -0.7735,  0.4327,  0.9820, -0.7971, -0.7338, -0.5301,\n",
      "          -0.6381, -0.4873,  0.9493, -0.9807, -0.9366, -0.9969,  0.2872,\n",
      "          -0.9563,  0.8347,  0.8798,  0.7663,  0.9711, -0.9980, -0.9689,\n",
      "           0.2148,  0.5466, -0.0893, -0.6144, -0.9914, -0.9612, -0.9856,\n",
      "           0.6748,  0.9988,  0.9987,  0.7713,  0.4883, -0.7335, -0.7726,\n",
      "           0.8958,  0.6939, -0.5401,  0.8186,  0.6191,  0.3271,  0.9419,\n",
      "          -0.8278,  0.2212,  0.9722,  0.9667, -0.1313,  0.8741, -0.9287,\n",
      "          -0.9553,  0.9913,  0.9646,  0.2215,  0.9241,  0.9841,  0.7220,\n",
      "          -0.9542,  0.5943,  0.9987,  0.9967, -0.9964, -0.9298,  0.9997,\n",
      "           0.6297,  0.9663, -0.9874, -0.9895,  0.0575, -0.4724,  0.8944,\n",
      "          -0.9565,  0.3947, -0.7296,  0.9633, -0.9982,  0.6855, -0.5607,\n",
      "           0.9973, -0.9984,  0.9977, -0.9595,  0.6489, -0.7726,  0.6567,\n",
      "          -0.9263,  0.7650,  0.9845,  0.6938,  0.9990,  0.9820, -0.9994,\n",
      "          -0.9840, -0.9467,  0.9666,  0.1556, -0.5262, -0.8814,  0.7268,\n",
      "          -0.9959, -0.9996, -0.9982,  0.8767,  0.0564,  0.7680,  0.9303,\n",
      "           0.6061, -0.3679,  0.9981,  0.6025,  0.9898, -0.6981, -0.4482,\n",
      "           0.9696, -0.9511,  0.8820,  0.1992, -0.9144,  0.8454, -0.5612,\n",
      "           0.8171, -0.9475,  0.1641,  0.9988, -0.9999, -0.6876, -0.0265,\n",
      "          -0.4660, -0.2772,  0.9732, -0.4843, -0.9082,  0.9829, -0.5336,\n",
      "          -0.8300, -1.0000,  0.4989,  0.5066, -0.9989,  0.9918, -0.9553,\n",
      "           0.9261,  0.9992,  0.9949, -0.7317, -0.3751, -0.8943, -0.2075,\n",
      "           0.9995, -0.2441,  0.9996,  0.5743,  0.9817,  0.9482, -0.9916,\n",
      "          -0.9999, -0.9929,  0.9940, -0.8937,  0.9658,  0.9990,  0.5942,\n",
      "           0.9055,  0.7808,  0.0304,  0.7486,  0.8227, -0.2054, -0.0228,\n",
      "           0.8619,  0.3581, -0.9816, -0.2651, -0.8738, -0.9674, -0.9918,\n",
      "           0.9735,  0.9981, -0.9974, -0.8460,  0.9736,  0.9524, -0.8538,\n",
      "           0.2161, -0.9986,  0.6557,  0.9389,  0.1046,  0.9774,  0.6581,\n",
      "          -0.1764, -0.2786,  0.9998, -0.0460,  0.7954,  0.8123, -0.8301,\n",
      "          -0.4127, -0.7524, -0.9857, -0.3358,  0.5324,  0.9875, -0.9986,\n",
      "          -0.1929, -0.9988, -0.9476, -0.9835, -0.8086,  0.3920,  0.9934,\n",
      "          -0.4766, -0.7614, -0.5626,  0.8726]]],\n",
      "       grad_fn=<IndexSelectBackward0>), tensor([[[-0.9611,  0.9938,  0.6189,  ..., -0.9463,  0.9177, -0.8873]],\n",
      "\n",
      "        [[-0.8274, -0.4605, -0.0836,  ..., -0.7084,  0.5285,  0.5221]],\n",
      "\n",
      "        [[ 0.9000,  0.7546, -0.3875,  ...,  0.3762, -0.3718, -0.7987]],\n",
      "\n",
      "        [[ 0.9866, -0.7455, -0.0893,  ..., -0.7377, -0.6293, -0.7959]]],\n",
      "       grad_fn=<IndexSelectBackward0>), tensor([[[-0.5896,  0.9707,  0.9904,  ...,  0.8618,  0.9983, -0.0508]],\n",
      "\n",
      "        [[-0.9724, -0.9283, -0.8078,  ..., -0.6561, -0.5679, -0.2312]],\n",
      "\n",
      "        [[-0.6119, -0.6597, -0.0835,  ..., -0.0980,  0.8839,  0.9232]],\n",
      "\n",
      "        [[ 0.3292,  0.0269,  0.5503,  ...,  0.0479,  0.6754,  0.2546]],\n",
      "\n",
      "        [[-0.1712,  0.2103,  0.6977,  ..., -0.7373,  0.8643, -0.5454]]],\n",
      "       grad_fn=<IndexSelectBackward0>)]\n",
      "Lhre care of yourself. The peoce of kolto tanks has jumped through the roof.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prevs,ins,outs = dataset[0]\n",
    "N_prevs = len(prevs)\n",
    "N_currents = len(ins)\n",
    "prevs = prevs.to(device)[None,:]\n",
    "ins = ins.to(device)[None,:]\n",
    "outs = outs.to(device)[None,:]\n",
    "prevs = add_positional_info(prevs)\n",
    "ins = add_positional_info(ins)\n",
    "print(prevs.shape)\n",
    "print(prevs)\n",
    "print(dataset.vec2str(prevs[0,:,0]))\n",
    "print(''.join( str(i) for i in prevs[0,:,1].tolist()[2:]))\n",
    "print(ins.shape)\n",
    "print(ins)\n",
    "print(dataset.vec2str(ins[0,:,0]))\n",
    "print(''.join( str(i) for i in ins[0,:,1].tolist()[2:]))\n",
    "encoder_tensor = encoder(prevs,[N_prevs])\n",
    "print(encoder_tensor.shape)\n",
    "print(encoder_tensor)\n",
    "pred, states = decoder(ins,encoder_tensor,[N_currents])\n",
    "print(states)\n",
    "print(dataset.vec2str(torch.exp(pred).squeeze(0).multinomial(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "29d9701b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 145, 2])\n",
      "tensor([[[ 1,  0],\n",
      "         [60,  0],\n",
      "         [39,  0],\n",
      "         [68,  0],\n",
      "         [68,  0],\n",
      "         [67,  0],\n",
      "         [ 2,  1],\n",
      "         [83,  1],\n",
      "         [71,  1],\n",
      "         [68,  1],\n",
      "         [ 2,  2],\n",
      "         [65,  2],\n",
      "         [68,  2],\n",
      "         [64,  2],\n",
      "         [82,  2],\n",
      "         [83,  2],\n",
      "         [ 2,  3],\n",
      "         [64,  3],\n",
      "         [77,  3],\n",
      "         [67,  3],\n",
      "         [ 2,  4],\n",
      "         [72,  4],\n",
      "         [83,  4],\n",
      "         [ 2,  5],\n",
      "         [86,  5],\n",
      "         [72,  5],\n",
      "         [75,  5],\n",
      "         [75,  5],\n",
      "         [ 2,  6],\n",
      "         [71,  6],\n",
      "         [68,  6],\n",
      "         [68,  6],\n",
      "         [67,  6],\n",
      "         [ 2,  7],\n",
      "         [88,  7],\n",
      "         [78,  7],\n",
      "         [84,  7],\n",
      "         [81,  7],\n",
      "         [ 2,  8],\n",
      "         [66,  8],\n",
      "         [64,  8],\n",
      "         [75,  8],\n",
      "         [75,  8],\n",
      "         [15,  0],\n",
      "         [ 2,  1],\n",
      "         [53,  1],\n",
      "         [64,  1],\n",
      "         [74,  1],\n",
      "         [68,  1],\n",
      "         [ 2,  2],\n",
      "         [85,  2],\n",
      "         [72,  2],\n",
      "         [79,  2],\n",
      "         [68,  2],\n",
      "         [81,  2],\n",
      "         [82,  2],\n",
      "         [ 2,  3],\n",
      "         [69,  3],\n",
      "         [81,  3],\n",
      "         [78,  3],\n",
      "         [76,  3],\n",
      "         [ 2,  4],\n",
      "         [83,  4],\n",
      "         [71,  4],\n",
      "         [68,  4],\n",
      "         [72,  4],\n",
      "         [81,  4],\n",
      "         [ 2,  5],\n",
      "         [75,  5],\n",
      "         [64,  5],\n",
      "         [72,  5],\n",
      "         [81,  5],\n",
      "         [ 2,  6],\n",
      "         [64,  6],\n",
      "         [77,  6],\n",
      "         [67,  6],\n",
      "         [ 2,  7],\n",
      "         [71,  7],\n",
      "         [64,  7],\n",
      "         [77,  7],\n",
      "         [70,  7],\n",
      "         [ 2,  8],\n",
      "         [83,  8],\n",
      "         [71,  8],\n",
      "         [68,  8],\n",
      "         [76,  8],\n",
      "         [ 2,  9],\n",
      "         [69,  9],\n",
      "         [81,  9],\n",
      "         [78,  9],\n",
      "         [76,  9],\n",
      "         [ 2, 10],\n",
      "         [64, 10],\n",
      "         [65, 10],\n",
      "         [78, 10],\n",
      "         [85, 10],\n",
      "         [68, 10],\n",
      "         [15,  0],\n",
      "         [ 2,  1],\n",
      "         [45,  1],\n",
      "         [68,  1],\n",
      "         [83,  1],\n",
      "         [ 2,  2],\n",
      "         [65,  2],\n",
      "         [75,  2],\n",
      "         [78,  2],\n",
      "         [78,  2],\n",
      "         [67,  2],\n",
      "         [ 2,  3],\n",
      "         [82,  3],\n",
      "         [66,  3],\n",
      "         [68,  3],\n",
      "         [77,  3],\n",
      "         [83,  3],\n",
      "         [ 2,  4],\n",
      "         [83,  4],\n",
      "         [71,  4],\n",
      "         [68,  4],\n",
      "         [ 2,  5],\n",
      "         [70,  5],\n",
      "         [81,  5],\n",
      "         [78,  5],\n",
      "         [84,  5],\n",
      "         [77,  5],\n",
      "         [67,  5],\n",
      "         [ 2,  6],\n",
      "         [78,  6],\n",
      "         [69,  6],\n",
      "         [ 2,  7],\n",
      "         [78,  7],\n",
      "         [84,  7],\n",
      "         [81,  7],\n",
      "         [ 2,  8],\n",
      "         [64,  8],\n",
      "         [77,  8],\n",
      "         [66,  8],\n",
      "         [68,  8],\n",
      "         [82,  8],\n",
      "         [83,  8],\n",
      "         [78,  8],\n",
      "         [81,  8],\n",
      "         [82,  8],\n",
      "         [15,  0],\n",
      "         [61,  0],\n",
      "         [ 0,  0]]])\n",
      "[Feed the beast and it will heed your call. Take vipers from their lair and hang them from above. Let blood scent the ground of our ancestors.]\n",
      "\n",
      "00001111222222333344455555666667777788888011111222222233333444444555556666777778888899999101010101010011112222223333334444555555566677778888888888000\n",
      "torch.Size([1, 62, 2])\n",
      "tensor([[[ 1,  0],\n",
      "         [60,  0],\n",
      "         [48,  0],\n",
      "         [65,  0],\n",
      "         [85,  0],\n",
      "         [72,  0],\n",
      "         [78,  0],\n",
      "         [84,  0],\n",
      "         [82,  0],\n",
      "         [75,  0],\n",
      "         [88,  0],\n",
      "         [ 2,  1],\n",
      "         [83,  1],\n",
      "         [71,  1],\n",
      "         [72,  1],\n",
      "         [82,  1],\n",
      "         [ 2,  2],\n",
      "         [86,  2],\n",
      "         [64,  2],\n",
      "         [82,  2],\n",
      "         [ 2,  3],\n",
      "         [78,  3],\n",
      "         [77,  3],\n",
      "         [66,  3],\n",
      "         [68,  3],\n",
      "         [ 2,  4],\n",
      "         [64,  4],\n",
      "         [ 2,  5],\n",
      "         [79,  5],\n",
      "         [75,  5],\n",
      "         [64,  5],\n",
      "         [66,  5],\n",
      "         [68,  5],\n",
      "         [ 2,  6],\n",
      "         [78,  6],\n",
      "         [69,  6],\n",
      "         [ 2,  7],\n",
      "         [70,  7],\n",
      "         [81,  7],\n",
      "         [68,  7],\n",
      "         [64,  7],\n",
      "         [83,  7],\n",
      "         [ 2,  8],\n",
      "         [81,  8],\n",
      "         [72,  8],\n",
      "         [83,  8],\n",
      "         [84,  8],\n",
      "         [64,  8],\n",
      "         [75,  8],\n",
      "         [ 2,  9],\n",
      "         [72,  9],\n",
      "         [76,  9],\n",
      "         [79,  9],\n",
      "         [78,  9],\n",
      "         [81,  9],\n",
      "         [83,  9],\n",
      "         [64,  9],\n",
      "         [77,  9],\n",
      "         [66,  9],\n",
      "         [68,  9],\n",
      "         [15,  0],\n",
      "         [61,  0]]])\n",
      "[Obviously this was once a place of great ritual importance.]\n",
      "000000000111112222333334455555566677777788888889999999999900\n",
      "torch.Size([1, 256])\n",
      "tensor([[-0.6969,  0.3581,  0.7228, -0.4099,  0.4117,  2.2309, -1.4551,  0.2322,\n",
      "          1.0760,  1.0002,  1.6513, -2.4673,  1.6382,  2.3641, -0.8666,  0.0565,\n",
      "          0.7366,  0.3041,  0.7873,  0.8065,  0.8882, -1.0400, -0.1616,  1.8352,\n",
      "          0.5081, -1.8623, -1.0121,  0.0675,  0.4463, -0.6874,  0.6409, -2.4500,\n",
      "          2.1946, -2.8124, -1.6665,  1.2143,  1.2518, -0.2524, -0.6735, -0.0281,\n",
      "          1.6259,  1.2950,  0.5065, -0.3492, -0.1674, -0.1572,  0.6068,  0.0531,\n",
      "          0.9795, -0.4909,  0.7533, -1.1028, -0.5638, -1.0598, -0.8965,  0.3327,\n",
      "          0.7046,  0.2086, -1.9639,  0.4051, -1.3566, -0.2619, -1.1882,  1.3238,\n",
      "          1.5155, -0.3305, -0.3802,  0.3073,  2.7188,  0.2904, -0.8715, -0.3473,\n",
      "          0.7784, -0.1045, -1.5417, -0.1743, -0.6490,  0.0944,  0.4151, -0.6395,\n",
      "          0.5243,  0.6365,  0.7889, -0.5086,  0.2531,  0.0712,  0.3307, -0.1014,\n",
      "         -1.1237,  0.0601, -1.2725, -0.7627,  0.2270,  0.9682,  0.3338,  1.6252,\n",
      "          0.7209,  0.8619, -0.6481, -1.0667, -0.2085,  0.9926, -0.2244, -0.9629,\n",
      "          2.4555, -0.3285,  0.5996, -2.2172,  1.8219, -0.9487, -1.5235,  2.3514,\n",
      "         -1.4048, -0.2483, -0.9590, -0.2600, -2.2617, -1.3348, -2.8496, -1.0423,\n",
      "         -0.0290,  0.1336, -0.1556, -0.4281,  1.7002,  0.6033, -0.6003,  0.9802,\n",
      "         -0.9540, -1.9605, -2.3575, -3.1360, -0.6082, -2.5407,  0.2401,  1.6979,\n",
      "          0.5106, -0.1086, -0.7892, -0.3640,  2.0732, -0.9852,  1.8620,  0.0798,\n",
      "          1.0875,  0.0964, -1.3397, -0.8687,  1.9184, -1.1506, -1.2380,  1.0672,\n",
      "         -2.3318,  2.3264,  0.3699,  1.5176,  0.5036, -1.2191, -0.3935, -0.1933,\n",
      "         -0.3528,  1.5621, -0.1320,  0.8944,  0.1477, -1.1450,  0.3715,  1.6021,\n",
      "          0.1206, -1.0325,  0.5331, -1.1274, -0.9401, -0.9386, -0.7625,  1.5295,\n",
      "         -0.1365, -1.0756,  1.0600, -1.0492, -1.4749,  0.3955, -1.4245, -0.7065,\n",
      "         -0.4948,  0.2452, -2.6989,  1.0709,  2.0549, -0.0194,  1.4071,  0.9495,\n",
      "          0.0803,  0.1238,  0.1753, -0.4732, -0.3573, -1.9774, -1.8354, -0.6422,\n",
      "         -2.2336, -3.4241, -1.9780, -1.2714,  2.1598,  1.9206, -0.2382,  1.9449,\n",
      "          2.0703,  1.3734,  2.1796, -1.9176, -2.2438,  0.8802,  1.0514, -2.4922,\n",
      "         -0.2287,  0.3118,  0.8858,  2.3086, -0.4775, -0.6196, -0.9972, -0.8904,\n",
      "          0.4374, -0.7003,  0.9544,  0.7895, -0.4142, -1.7924, -0.6224, -0.0914,\n",
      "         -1.0068, -0.7484, -1.8461,  0.4109, -0.9951,  0.2872, -1.6920, -0.0539,\n",
      "         -2.1964,  0.1454, -0.2206,  0.0570,  0.4579,  1.3236,  0.1288,  0.1797,\n",
      "         -1.0900,  2.8176,  0.1566, -0.3143,  1.5882,  0.0404,  2.0326, -1.8094]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "[tensor([[[-0.6244,  0.8074,  0.9992,  0.7878, -0.9999,  0.9728,  0.9900,\n",
      "          -0.9058,  0.2115,  0.9162, -0.9989, -0.9989,  0.1189,  0.8059,\n",
      "          -0.7777, -0.9981,  0.9988,  0.9988,  0.9352,  0.7760, -0.9931,\n",
      "           0.7460, -0.6399, -0.8602,  0.9579, -0.9200, -0.6213, -0.9867,\n",
      "           0.9529, -0.9936,  0.7325, -0.9493, -0.9868,  0.9490,  0.9872,\n",
      "           0.9132, -0.6344,  0.2490,  0.6697,  0.6125,  0.0992, -0.9998,\n",
      "           0.7681,  0.8497, -0.9697,  0.9278, -0.8224,  0.7203,  0.9838,\n",
      "           0.9063,  0.8296,  0.2027,  0.8872,  0.8760, -0.6470, -0.9998,\n",
      "          -0.5734, -0.9997,  0.9962,  0.9994, -0.0504,  0.0156,  0.9736,\n",
      "          -0.7486, -0.2931,  0.9993, -0.9668,  0.9779,  0.6894, -0.9418,\n",
      "          -0.9996,  0.9567, -0.9950,  0.7340, -0.9340, -0.7411, -0.6756,\n",
      "          -0.0475,  0.7239, -0.7709, -0.7458,  0.8330,  0.9992, -0.5148,\n",
      "          -0.0386,  0.2720,  0.5104,  0.7802, -0.4789, -0.6815,  0.8474,\n",
      "          -0.6990,  0.9277,  0.9914, -0.9562,  0.4611, -0.9758,  0.9441,\n",
      "           0.8103,  0.8479, -0.7409, -0.1501,  0.9691, -0.1759, -0.9778,\n",
      "          -0.9221,  0.9926,  0.4970,  0.9567, -0.9366,  0.5437,  0.2273,\n",
      "          -0.9155,  0.9997,  0.9594, -1.0000, -0.7301, -0.8897, -0.1700,\n",
      "          -0.3174, -0.9152,  0.7348,  0.9991, -0.3513,  1.0000,  0.2989,\n",
      "           0.9565, -0.0512,  0.9703,  0.9769,  0.9624, -0.9966,  0.8061,\n",
      "           0.9827, -0.5795, -0.5829,  0.8451,  0.6867, -0.9986, -0.6077,\n",
      "          -0.9285, -0.3186, -0.4819,  0.8709,  0.9741,  0.9998,  0.7670,\n",
      "           0.9930,  0.0402,  0.9835, -0.9565,  0.9992,  0.3895, -0.9211,\n",
      "          -0.4245,  0.9510,  0.9623,  0.9752,  0.9871,  0.9992, -0.9872,\n",
      "          -0.9488, -0.9039,  0.9741, -0.9981, -0.6277,  0.7400,  0.8582,\n",
      "          -0.6538, -0.0239,  0.9983, -0.1512, -0.9765,  0.6308,  0.8730,\n",
      "          -0.8258, -0.8084, -0.9105, -0.4066,  0.0699, -0.0790,  0.9920,\n",
      "           0.9860,  0.7259, -0.9356, -0.6220, -0.9560, -0.9995,  0.5527,\n",
      "          -0.5421,  0.1424,  0.9866,  0.9987, -0.8815,  0.9386,  0.0465,\n",
      "           0.9958,  0.8684, -0.9827,  0.9530, -0.9530,  0.5875,  0.1235,\n",
      "          -0.9628, -0.8483,  0.0804, -0.7438,  0.9033,  0.7756, -0.9924,\n",
      "           0.0814, -0.2113, -0.8996, -0.8797, -0.8926,  0.6209, -0.8920,\n",
      "           0.9673, -0.9975,  0.9929, -0.2987, -0.9554,  0.9993, -0.9993,\n",
      "           0.9725, -0.9144,  0.4724,  0.5811, -0.9943,  0.8915,  0.0268,\n",
      "           0.9918, -0.9559,  0.1845, -0.2878, -0.9934, -0.9601,  0.9038,\n",
      "           0.9957, -0.9838, -0.7854, -0.8678, -0.8224, -0.1185, -0.9342,\n",
      "           0.9706, -0.9627,  0.9172, -0.5036,  0.5641, -0.2044, -0.9993,\n",
      "          -0.9831,  0.8286, -0.9386, -0.7263]]],\n",
      "       grad_fn=<IndexSelectBackward0>), tensor([[[-0.5371, -0.9767,  0.4116, -0.9987, -0.9947,  0.9986,  0.3926,\n",
      "          -0.9617,  0.7408, -0.1803,  0.1975, -0.4824,  0.9785, -0.4235,\n",
      "          -0.9874,  0.9216,  0.7090, -0.9549,  0.8504, -0.9956,  0.7923,\n",
      "           0.8362, -0.9893, -0.0383, -0.1902, -0.6575,  0.7490, -0.9930,\n",
      "           0.1431,  0.6080,  0.9113,  0.1023,  0.8222, -0.9994, -0.9512,\n",
      "           0.5495, -0.4953,  0.5657, -0.3065,  0.9582, -0.9997,  0.9884,\n",
      "          -0.1841, -0.9617,  0.9931, -0.9995,  0.9901, -0.4540, -0.9946,\n",
      "           0.7601,  0.9472,  0.9892, -0.2330, -0.4537,  0.9540,  0.3676,\n",
      "           0.0220,  0.2827, -0.6110, -0.4866, -0.9951, -0.9666, -0.0868,\n",
      "          -0.8356,  0.2197,  0.9488, -0.9846,  0.9992,  0.6103, -0.9912,\n",
      "           0.7025,  0.9841, -0.6526,  0.9905,  0.8783, -0.9504,  0.9935,\n",
      "           0.2299,  0.8801,  0.9914,  0.2158, -0.9996, -0.4335,  0.2207,\n",
      "          -0.2935, -0.9158, -0.8909,  0.8074, -0.9731, -0.2301,  0.1553,\n",
      "          -0.6893,  0.9906, -0.7044,  0.8172, -0.2151, -0.9996,  0.9551,\n",
      "          -0.9411, -0.5647, -0.9878, -0.2469,  0.8961, -0.1858, -0.9285,\n",
      "          -0.9375, -0.3117,  0.6477,  0.9373,  0.9195,  0.5907, -0.9955,\n",
      "          -0.9969,  0.8030, -0.0551, -0.9398, -0.8312,  0.5136, -0.7977,\n",
      "           0.6974,  0.4720,  0.7163,  0.3331, -0.8547, -0.9927, -0.1832,\n",
      "          -0.8137,  0.9847,  0.7630, -0.9766, -0.8823, -0.2499,  0.9591,\n",
      "           0.9913,  0.6739,  0.3572,  0.0069,  0.6447,  0.9940,  0.3785,\n",
      "           0.5537, -0.9944,  0.1931, -0.3694, -0.9571, -0.4211, -0.3599,\n",
      "          -0.1350,  0.9609,  0.9398,  0.9838,  0.7222, -0.9634,  0.9998,\n",
      "           0.9021,  0.9647, -0.8059,  0.4975,  0.9280,  0.9558,  0.9047,\n",
      "           0.3567, -0.0953,  0.4311,  0.7840, -0.0782,  0.0439,  0.8782,\n",
      "          -0.9542,  0.8526,  0.9905,  0.0809,  0.8793,  0.9950,  0.8671,\n",
      "           0.9995,  0.6961, -0.3475,  0.9992,  0.9512,  0.9768, -0.9987,\n",
      "           0.8471, -0.5285, -0.9965, -0.1786, -0.9254, -0.0672,  0.9655,\n",
      "          -0.9938, -0.8743, -0.6901,  0.9864,  0.9909,  0.8496, -0.4922,\n",
      "           0.1591, -0.7802,  0.6163, -0.8182, -0.7241,  0.8307, -0.0099,\n",
      "          -0.9906, -0.8681, -0.9389, -0.4366,  0.9997, -0.9006,  0.2865,\n",
      "           0.9979,  0.9961, -0.7892, -0.9031,  0.6063, -0.8747,  0.8707,\n",
      "          -0.1134, -0.5802,  0.8318, -0.9791, -0.7530, -0.5907, -0.3780,\n",
      "           0.9993, -0.7124,  0.7236,  0.2817, -0.8649,  0.7826,  0.9931,\n",
      "           0.4582,  0.9826,  0.9975,  0.9635, -0.9715,  0.6581, -0.6626,\n",
      "          -0.9044, -0.9965, -0.6032,  0.8140,  0.7083,  0.9695, -0.3356,\n",
      "           0.9874,  0.7585, -0.2693,  0.9402, -0.4781,  1.0000,  0.9287,\n",
      "          -0.6921, -0.0227,  0.6389,  0.9973]],\n",
      "\n",
      "        [[ 0.9906,  0.9502,  0.7612, -0.8258,  0.5251, -0.4056,  0.0623,\n",
      "           0.9255, -0.0749, -0.3024, -0.6485,  0.8846, -0.1848, -0.4992,\n",
      "          -0.2268, -0.6947, -0.9730,  0.5592, -0.1698,  0.8675,  0.6543,\n",
      "           0.0035, -0.5645,  0.7458,  0.8929,  0.0285,  0.9721,  0.5520,\n",
      "          -0.2240, -0.6331,  0.8436,  0.9691, -0.9449, -0.1621,  0.7004,\n",
      "           0.1520, -0.7327, -0.9433,  0.6226,  0.9584, -0.2999,  0.5667,\n",
      "          -0.9745, -0.9967,  0.9639,  0.9862,  0.3989,  0.3071, -0.2209,\n",
      "           0.9717,  0.1869,  0.9697, -0.2568,  0.7343, -0.4340,  0.7725,\n",
      "           0.9682,  0.6385, -0.6233, -0.8839,  0.8284,  0.1710, -0.2417,\n",
      "          -0.3368,  0.9793, -0.9042,  0.6635,  0.8843, -0.4802,  0.8885,\n",
      "          -0.9947,  0.9612,  0.9219,  0.2205,  0.9988,  0.8013,  0.5006,\n",
      "          -0.2844, -0.7408, -0.9341, -0.3280, -0.9026,  0.8657,  0.9950,\n",
      "          -0.5987, -0.4807, -0.9353,  0.7511,  0.9992, -0.9233,  0.3922,\n",
      "           0.2165,  0.1768, -0.4453, -0.4049,  0.1157, -0.5788, -0.8774,\n",
      "           0.9873,  0.9443,  0.8901,  0.1002, -0.9828, -0.1389, -0.1056,\n",
      "          -0.7884,  0.9917, -0.2642,  0.2323,  0.8741, -0.8982,  0.4081,\n",
      "           0.9991,  0.9498, -0.6936, -0.5119, -0.6355, -0.9969, -0.1617,\n",
      "          -0.4922,  0.3542,  0.7503, -0.7384, -0.0314, -0.5985,  0.8285,\n",
      "           0.1725, -0.5717, -0.3259, -0.5428,  0.5936, -0.7912,  0.6910,\n",
      "           0.3183, -0.9831,  0.9985, -0.6300,  0.9438,  0.6108, -0.9923,\n",
      "           0.4616,  0.9348,  0.6670, -0.3006, -0.3013, -0.7222, -0.9643,\n",
      "          -0.5677,  0.3543,  0.8940,  0.1499, -0.9962,  0.8811, -0.9623,\n",
      "           0.6631,  0.8090, -0.3933, -0.0764, -0.9505, -0.8584, -0.9811,\n",
      "          -0.0436,  0.6006, -0.9217,  0.9265,  0.4781, -0.9002, -0.3949,\n",
      "          -0.3358, -0.6387, -0.1755,  0.3125, -0.3990,  0.1791,  0.5353,\n",
      "           0.7548,  0.8305, -0.9544,  0.6647, -0.9756, -0.2735, -0.9301,\n",
      "           0.9756, -0.3791,  0.9444, -0.9363,  0.2239, -0.8122, -0.9933,\n",
      "           0.9488, -0.2990, -0.9319, -0.0815, -0.9919, -0.7547, -0.9182,\n",
      "           0.8227,  0.9020,  0.9377,  0.4049, -0.2494,  0.7615, -0.3272,\n",
      "           0.8137,  0.1584,  0.6691,  0.7954, -0.9947,  0.9868, -0.0851,\n",
      "          -0.6197, -0.9953, -0.7355,  0.7795, -0.9332, -0.8101, -0.0802,\n",
      "          -0.3762, -0.4304, -0.9044,  0.7697,  0.4957, -0.4128, -0.6892,\n",
      "           0.9136,  0.7452,  0.0971,  0.8910,  0.1914, -0.8784,  0.1221,\n",
      "          -0.9474,  0.2219,  0.9582,  0.2503, -0.9542,  0.9679, -0.9994,\n",
      "          -0.9289,  0.4877,  0.6696,  0.3928, -0.8545,  0.6965, -0.9891,\n",
      "          -0.1526,  0.0722, -0.3438,  0.4526, -0.1053, -0.4567, -0.5209,\n",
      "          -0.5791, -0.7147, -0.9720, -0.7976]]],\n",
      "       grad_fn=<IndexSelectBackward0>), tensor([[[ 8.0326e-01, -6.0632e-01, -3.0964e-01,  8.3900e-01,  5.4088e-01,\n",
      "           9.9109e-01,  4.4358e-01, -9.1403e-01,  9.0594e-01, -6.9502e-01,\n",
      "          -6.2695e-01,  9.8869e-01,  5.8326e-01,  9.7901e-01, -2.3796e-01,\n",
      "           4.2478e-01,  9.1025e-01,  4.6231e-01,  9.5966e-01,  9.9875e-01,\n",
      "           8.3033e-01, -9.7171e-01,  9.6095e-01, -6.8511e-01,  7.8679e-01,\n",
      "           6.8956e-01, -7.6591e-01, -9.6324e-01, -1.2269e-01,  8.9435e-01,\n",
      "          -4.2890e-01, -9.9974e-01, -8.0966e-01, -4.0103e-02,  7.8259e-01,\n",
      "          -6.2109e-01, -9.9719e-01,  8.1199e-01, -9.6854e-01,  5.9853e-01,\n",
      "           9.8679e-01,  9.5704e-01, -9.5673e-01,  6.9699e-01,  7.1661e-01,\n",
      "          -9.7448e-02,  9.6104e-01,  9.2097e-01,  5.9575e-02,  4.2774e-01,\n",
      "           7.1700e-01, -4.8833e-01, -9.5867e-01,  9.3380e-01, -7.0331e-01,\n",
      "           6.1363e-01,  2.7459e-01,  9.3339e-01, -9.7716e-01,  9.8693e-01,\n",
      "          -8.1214e-01, -9.6270e-01, -9.9237e-01, -9.8579e-01,  6.1234e-01,\n",
      "           3.0891e-04, -9.9788e-01, -9.0354e-01,  9.1065e-01, -5.3648e-02,\n",
      "           4.2738e-01, -9.8933e-01,  9.8679e-01, -8.9441e-01, -9.3498e-01,\n",
      "           9.7724e-01,  9.9996e-01,  9.9976e-01, -9.9180e-01, -9.7559e-01,\n",
      "          -9.9558e-01,  9.8005e-01,  4.4873e-01, -9.7882e-01, -3.0312e-01,\n",
      "           9.9758e-01, -6.4377e-01, -1.0262e-01, -6.6579e-01, -9.9870e-01,\n",
      "          -5.5388e-01, -9.2574e-01,  9.8021e-01,  9.3161e-01, -9.5666e-01,\n",
      "          -3.7773e-01,  9.6721e-01, -9.2172e-01, -9.3787e-01,  3.1649e-01,\n",
      "          -6.3500e-01,  2.3388e-01,  9.9933e-01, -9.3874e-01,  9.8772e-01,\n",
      "           7.2618e-01,  2.3272e-01, -9.9459e-01,  4.9147e-01,  6.3955e-02,\n",
      "           9.9299e-01,  9.6221e-01,  9.9895e-01,  8.5643e-01,  8.2718e-01,\n",
      "          -6.8009e-01,  7.4092e-01,  9.7292e-01,  8.2824e-01,  6.5389e-01,\n",
      "          -8.7987e-01,  8.0957e-01,  9.6838e-01, -1.6504e-01,  8.8627e-01,\n",
      "           9.9651e-01,  5.4401e-01,  9.8536e-01,  6.1526e-01, -5.2417e-01,\n",
      "          -4.1899e-01,  9.4903e-04, -2.1938e-01, -5.0871e-01,  8.2982e-01,\n",
      "           3.4426e-01, -9.9594e-01,  3.3568e-01,  9.8942e-01, -6.7458e-01,\n",
      "          -5.6105e-01, -8.6546e-01,  9.9999e-01,  7.9697e-01,  9.9920e-01,\n",
      "           6.9262e-01, -9.9765e-01, -9.9855e-01,  6.4906e-01, -9.7942e-01,\n",
      "          -8.3238e-01,  6.8244e-01,  9.4696e-01,  7.1502e-01, -8.6960e-01,\n",
      "          -3.4755e-01,  5.4181e-01,  9.6316e-01, -6.1771e-01, -9.8733e-01,\n",
      "          -8.5291e-01,  2.9487e-01, -9.0768e-01,  6.7845e-01, -9.9067e-01,\n",
      "           8.9384e-01,  7.4757e-01, -9.3264e-01, -9.8621e-01,  9.2444e-02,\n",
      "          -9.1424e-01,  5.6000e-01, -8.5464e-01, -1.3592e-01,  8.0037e-01,\n",
      "           9.8037e-01,  7.9416e-01, -1.9403e-01,  3.5249e-02,  8.9478e-01,\n",
      "          -1.8307e-01,  9.8957e-01,  8.9928e-01, -9.6672e-01, -6.9819e-01,\n",
      "          -6.1939e-01, -7.4751e-01, -2.8584e-01, -4.0302e-01,  8.4914e-01,\n",
      "          -9.6422e-01, -9.9471e-01,  8.9968e-01, -9.5927e-01, -9.8091e-01,\n",
      "           5.6940e-01,  4.1019e-01, -7.7295e-03,  9.9993e-01, -2.6066e-01,\n",
      "          -9.9004e-01,  3.6189e-01,  9.7775e-01, -9.3035e-01, -7.6389e-01,\n",
      "          -7.0706e-01, -8.4233e-01, -7.9852e-01,  9.9044e-01, -9.9950e-01,\n",
      "          -9.9921e-01, -5.2625e-01,  7.4502e-02, -9.9077e-01,  3.4724e-02,\n",
      "           3.4843e-01,  9.9489e-01, -9.8434e-01, -2.7169e-01,  9.7452e-01,\n",
      "           6.3077e-01,  1.4903e-01, -7.8099e-01,  6.8640e-01, -8.9621e-01,\n",
      "           1.2945e-01,  5.9115e-02,  4.3429e-01,  3.5005e-02,  5.9660e-01,\n",
      "           3.0815e-01,  8.8075e-01,  9.9542e-01, -4.5973e-01,  9.9788e-01,\n",
      "          -9.9692e-01, -9.4553e-01, -3.6529e-01, -3.4651e-01,  7.4866e-01,\n",
      "          -1.6226e-03,  7.4518e-01,  3.1257e-01, -9.2846e-01,  2.7066e-01,\n",
      "          -4.1033e-01, -3.4307e-01, -8.9736e-01, -9.6999e-01, -9.9219e-01,\n",
      "           6.8628e-01, -9.4855e-01, -9.8611e-01,  7.1326e-01,  3.6363e-01,\n",
      "           2.6211e-01]],\n",
      "\n",
      "        [[-5.7423e-01,  6.6845e-01, -9.6392e-01, -5.8989e-01, -9.6900e-01,\n",
      "          -6.1800e-01, -7.4576e-01, -6.5949e-01,  9.5927e-01,  8.7247e-01,\n",
      "          -3.6308e-01,  6.1017e-01, -9.8373e-01, -9.9470e-01, -9.9982e-01,\n",
      "          -5.8347e-01, -4.2401e-02, -9.4081e-01,  9.8745e-01, -7.8519e-01,\n",
      "          -8.2343e-01,  9.7283e-01, -9.4674e-01,  7.8116e-01,  8.0151e-01,\n",
      "          -8.2639e-01,  8.6005e-01, -7.4712e-01, -9.4968e-01, -1.2114e-01,\n",
      "          -7.0349e-01,  9.2096e-01, -9.1960e-01,  9.6603e-01, -9.5726e-01,\n",
      "           8.3398e-01, -7.9601e-01,  8.4232e-01, -9.7648e-01, -9.9912e-01,\n",
      "          -6.0242e-01,  8.8621e-01, -9.7888e-01, -3.0361e-01,  4.3090e-01,\n",
      "           7.7597e-01, -7.0811e-01, -5.9721e-01,  1.9564e-01,  1.9093e-01,\n",
      "          -7.3122e-01, -9.9044e-01, -9.9991e-01, -6.3422e-01,  9.3062e-01,\n",
      "          -9.9110e-01, -4.8149e-01,  9.1136e-01,  5.0297e-01, -6.1778e-01,\n",
      "           8.5739e-01,  6.3113e-01, -2.0804e-01,  5.1001e-02,  7.3634e-01,\n",
      "           4.7634e-01, -4.6409e-01,  4.0581e-01,  7.7864e-01, -1.3799e-01,\n",
      "          -9.9154e-01, -2.0034e-01, -9.9214e-01,  4.2126e-01, -8.7258e-01,\n",
      "           3.5741e-01,  1.9061e-01,  9.3982e-01,  9.8304e-01, -4.4029e-01,\n",
      "           3.0255e-01, -7.5414e-01,  8.8987e-01,  9.6876e-01, -3.4137e-01,\n",
      "           2.4787e-01,  9.7016e-01,  7.5681e-01,  5.0166e-01, -9.5089e-01,\n",
      "           1.8455e-04,  9.4523e-01, -8.8643e-01,  6.7663e-01,  1.2817e-01,\n",
      "          -9.5111e-01,  3.9352e-01,  9.8988e-01,  6.5466e-01, -8.2287e-01,\n",
      "           9.4993e-01,  7.8038e-01, -4.0252e-02, -6.8420e-01, -8.8430e-01,\n",
      "          -4.1332e-01, -9.8349e-01, -2.8062e-01, -2.9490e-01,  8.3444e-01,\n",
      "          -7.4051e-01,  2.0733e-01, -6.3994e-01, -2.5896e-01,  8.9645e-01,\n",
      "          -6.7639e-01, -9.8404e-01,  2.0084e-01, -9.8522e-01, -9.9771e-01,\n",
      "          -7.0053e-01, -1.3883e-01,  5.3233e-01,  7.4073e-01,  7.5850e-01,\n",
      "          -4.2159e-01, -6.9812e-01, -9.9986e-01,  5.2053e-01,  4.0919e-01,\n",
      "           8.6461e-01, -3.0800e-01, -9.3255e-01,  9.2776e-01,  1.8327e-01,\n",
      "          -2.4569e-01, -9.9872e-01, -9.9476e-01, -8.3848e-01,  9.6996e-01,\n",
      "          -9.4253e-01,  9.0562e-01,  7.2215e-01, -1.7303e-01, -2.7530e-01,\n",
      "          -4.0134e-01, -3.5831e-01,  9.0678e-01,  9.9703e-01,  1.8403e-01,\n",
      "          -5.1692e-01,  5.4683e-01, -5.2896e-01, -8.6472e-01, -4.0983e-01,\n",
      "          -6.9866e-02, -9.5644e-01,  7.8668e-01,  7.3746e-01, -9.1635e-01,\n",
      "           8.2609e-01, -9.8648e-01,  7.7067e-01,  7.2956e-01, -2.3835e-01,\n",
      "           8.9018e-01,  5.5161e-01, -9.9676e-01,  9.2750e-01, -6.7817e-01,\n",
      "           8.6718e-01,  3.7989e-01,  5.0498e-01,  8.9526e-01,  4.1604e-01,\n",
      "           1.3573e-01,  8.6779e-01,  4.9219e-01,  6.1552e-01,  4.3220e-01,\n",
      "           7.3211e-01,  9.9958e-01,  9.2844e-01,  5.9156e-01, -4.3420e-01,\n",
      "           4.1421e-01, -5.2476e-01,  2.1562e-01,  9.9643e-01,  7.3819e-02,\n",
      "          -8.2229e-02,  3.6632e-01, -9.6856e-01, -9.1325e-01, -2.3747e-01,\n",
      "           9.9639e-01,  9.5116e-01,  2.5134e-01, -4.5783e-01, -2.3258e-01,\n",
      "           9.9281e-01,  8.6033e-01,  9.9485e-01,  9.5051e-01, -4.4747e-01,\n",
      "          -1.4709e-01,  9.9951e-01, -9.9746e-01,  9.9923e-01,  8.7143e-01,\n",
      "          -8.9734e-01, -7.3833e-01,  1.1733e-01,  9.7900e-01,  6.5949e-02,\n",
      "          -9.2212e-01, -9.9576e-01, -9.9978e-01, -3.4892e-01, -5.8926e-01,\n",
      "           2.5609e-01,  2.7767e-01, -9.9562e-01, -5.2837e-02, -2.1726e-01,\n",
      "          -9.9833e-01, -9.7478e-01, -7.0411e-01, -5.5684e-01, -1.2340e-01,\n",
      "          -8.9546e-01,  2.9603e-01,  3.3739e-01, -7.4320e-01,  8.3150e-01,\n",
      "           7.5215e-01, -2.0105e-01,  3.3873e-01, -9.9855e-01, -9.9502e-01,\n",
      "          -9.9537e-01, -4.7729e-01, -2.7629e-01, -5.2138e-01, -1.7286e-01,\n",
      "           1.3309e-01,  6.0469e-01, -9.9958e-01,  9.2103e-01,  7.5947e-01,\n",
      "           8.3267e-01, -7.4562e-01,  7.3047e-01,  9.6233e-01, -9.9647e-01,\n",
      "          -8.8143e-01]],\n",
      "\n",
      "        [[ 8.6572e-01,  9.6463e-01, -9.6748e-01,  9.0338e-01, -8.8393e-01,\n",
      "          -9.9771e-01, -9.3311e-02,  4.9298e-01, -8.9870e-01,  2.4987e-01,\n",
      "          -9.6459e-01,  9.7342e-01, -8.7621e-01,  2.7512e-01,  8.9360e-01,\n",
      "           9.9559e-01,  4.2078e-01, -9.6473e-01,  3.9511e-01,  9.9306e-01,\n",
      "           5.8064e-01, -8.5548e-01,  6.9920e-01,  7.1721e-01,  6.8175e-01,\n",
      "           4.7997e-01, -5.3422e-01, -4.5159e-01,  7.3310e-01, -8.7708e-01,\n",
      "           1.5494e-01, -4.4922e-01,  3.9319e-01,  5.2311e-01,  9.9113e-01,\n",
      "           8.8909e-01,  6.3881e-01, -8.7150e-01, -8.8368e-01, -5.5864e-01,\n",
      "          -6.3880e-01,  9.9971e-01,  8.4326e-01, -7.6993e-01,  9.5559e-01,\n",
      "           7.6834e-01, -2.4364e-01,  5.5900e-01, -9.3934e-01, -9.9331e-01,\n",
      "           6.0471e-01,  4.0820e-01, -7.8763e-01,  9.1325e-01, -9.3203e-01,\n",
      "          -9.5559e-01, -9.4066e-01,  9.9401e-01,  9.3816e-01,  8.7090e-01,\n",
      "          -6.6595e-01, -9.9971e-01,  9.9965e-01,  6.5965e-01,  7.8078e-01,\n",
      "          -8.4329e-01, -9.9632e-01, -8.2950e-01, -8.0151e-01, -9.8666e-01,\n",
      "           9.2971e-01,  7.0190e-01,  9.9960e-01,  1.2848e-01,  9.8897e-01,\n",
      "          -5.0785e-01, -9.0511e-01, -3.7062e-01, -1.6707e-01,  8.9480e-01,\n",
      "           7.4516e-01,  8.1342e-01, -9.0935e-01, -6.7218e-01,  9.5977e-01,\n",
      "          -9.4981e-01,  6.2076e-01,  6.7144e-01,  7.2455e-01,  9.8695e-01,\n",
      "           9.2852e-01,  9.0914e-01, -6.3180e-01,  9.7582e-01,  9.9720e-01,\n",
      "           9.2542e-01,  9.1963e-01,  8.6860e-01, -7.5162e-02, -6.1508e-01,\n",
      "           9.9534e-01,  9.9721e-01, -9.8159e-01, -6.1638e-01, -9.5718e-01,\n",
      "          -8.5150e-01,  9.8132e-01, -9.9306e-01, -8.7810e-01,  2.4872e-02,\n",
      "           7.8503e-02, -7.6766e-01, -6.8568e-01,  5.8940e-01, -3.5113e-03,\n",
      "           9.9632e-01, -7.7032e-01, -9.1114e-01, -9.7777e-01,  9.9592e-01,\n",
      "          -9.8208e-01,  9.9425e-01, -6.1670e-01,  8.9600e-01,  8.7500e-01,\n",
      "           9.7832e-01,  7.8795e-01, -7.1111e-01,  1.2652e-03,  9.8864e-01,\n",
      "          -7.5011e-01,  7.6799e-01, -9.3274e-01, -5.9545e-01,  9.2939e-01,\n",
      "           8.7125e-02, -7.2578e-01,  6.6792e-01, -5.1921e-01,  5.5051e-01,\n",
      "          -4.7401e-01,  9.9908e-01, -9.9985e-01,  9.9652e-01, -9.9095e-01,\n",
      "          -9.9862e-01,  9.6009e-01,  7.2249e-01, -9.5507e-01,  9.7104e-01,\n",
      "           9.1968e-01, -7.7659e-01, -9.6013e-01, -9.8333e-01,  9.9988e-01,\n",
      "          -8.4337e-01,  7.5881e-01, -8.1290e-01, -8.2538e-01,  8.3066e-01,\n",
      "          -9.3144e-01,  1.6141e-01, -5.4591e-01,  3.3915e-01, -9.9729e-01,\n",
      "          -9.9525e-01, -9.5137e-01,  5.7221e-01,  4.0023e-01, -9.6496e-01,\n",
      "           9.9937e-01, -9.9316e-01, -5.9384e-01,  9.7709e-01, -4.7211e-01,\n",
      "          -9.5866e-01, -9.9987e-01, -5.8421e-01, -9.7093e-01, -9.8021e-01,\n",
      "          -6.2478e-01, -8.3144e-01,  9.9999e-01,  9.8332e-01,  6.9768e-01,\n",
      "           3.5441e-01,  9.7122e-01, -9.1338e-01, -7.4475e-01, -4.9963e-01,\n",
      "          -4.8401e-01,  8.8457e-01, -6.0884e-01,  2.5682e-01,  8.2879e-01,\n",
      "          -5.9395e-01, -9.9545e-01, -9.9940e-01,  9.9435e-01, -9.2766e-01,\n",
      "           2.0367e-01,  9.9781e-01, -7.4008e-01,  8.6777e-01,  9.5738e-01,\n",
      "           3.4270e-01, -9.6325e-01,  2.2350e-01, -2.0874e-01, -1.6070e-01,\n",
      "          -8.2677e-01,  9.7539e-01,  9.7744e-01, -2.9050e-01,  1.0661e-01,\n",
      "          -8.5078e-01, -8.6266e-01,  5.4073e-03,  9.7513e-01, -2.9206e-01,\n",
      "          -3.0305e-01, -9.0059e-01,  8.4817e-01,  1.8238e-01,  5.9737e-01,\n",
      "           9.9151e-01,  3.9889e-01, -5.7022e-01,  1.8009e-01,  9.2972e-01,\n",
      "           9.0054e-01,  9.8614e-01,  3.6913e-01,  9.8726e-01, -9.6789e-01,\n",
      "          -8.9054e-01,  8.8297e-01, -1.5382e-01, -4.3169e-01, -6.2694e-02,\n",
      "          -9.6640e-01,  5.1382e-01,  1.1583e-01,  9.5873e-01,  2.2507e-01,\n",
      "          -7.3717e-01, -9.4733e-01, -6.7347e-01,  9.6041e-01, -3.1545e-01,\n",
      "           3.7911e-01,  9.9979e-01,  9.4526e-01, -9.6152e-01, -9.8505e-01,\n",
      "           9.9800e-01]]], grad_fn=<IndexSelectBackward0>), tensor([[[-0.8137,  0.8847,  0.9647,  ..., -0.9853,  0.8218, -0.9823]],\n",
      "\n",
      "        [[-0.5329,  0.9277,  0.7120,  ..., -0.8740,  0.5660,  0.1972]],\n",
      "\n",
      "        [[ 0.2929,  0.3390,  0.3873,  ...,  0.9483,  0.8406, -0.4662]],\n",
      "\n",
      "        [[ 0.7087,  0.9939, -0.9650,  ...,  0.9356, -0.7360, -0.7149]]],\n",
      "       grad_fn=<IndexSelectBackward0>), tensor([[[-0.9712,  0.9482,  0.9962,  ..., -0.6604,  0.1209,  0.2749]],\n",
      "\n",
      "        [[ 0.8330, -0.9433,  0.1101,  ...,  0.6898, -0.3565,  0.8155]],\n",
      "\n",
      "        [[-0.8976,  0.1791,  0.8005,  ..., -0.6778, -0.0331, -0.5668]],\n",
      "\n",
      "        [[-0.2800, -0.4250, -0.3975,  ...,  0.2173, -0.9861, -0.2168]],\n",
      "\n",
      "        [[ 0.9849,  0.9917,  0.4402,  ...,  0.2399,  0.4324, -0.9689]]],\n",
      "       grad_fn=<IndexSelectBackward0>)]\n",
      "[Obviously this was once a place of great ritual importance.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prevs,ins,outs = dataset[len(dataset)-1]\n",
    "N_prevs = len(prevs)\n",
    "N_currents = len(ins)\n",
    "prevs = prevs.to(device)[None,:]\n",
    "ins = ins.to(device)[None,:]\n",
    "outs = outs.to(device)[None,:]\n",
    "prevs = add_positional_info(prevs)\n",
    "ins = add_positional_info(ins)\n",
    "print(prevs.shape)\n",
    "print(prevs)\n",
    "print(dataset.vec2str(prevs[0,:,0]))\n",
    "print(''.join( str(i) for i in prevs[0,:,1].tolist()[2:]))\n",
    "print(ins.shape)\n",
    "print(ins)\n",
    "print(dataset.vec2str(ins[0,:,0]))\n",
    "print(''.join( str(i) for i in ins[0,:,1].tolist()[2:]))\n",
    "encoder_tensor = encoder(prevs,[N_prevs])\n",
    "print(encoder_tensor.shape)\n",
    "print(encoder_tensor)\n",
    "pred, states = decoder(ins,encoder_tensor,[N_currents])\n",
    "print(states)\n",
    "print(dataset.vec2str(torch.exp(pred).squeeze(0).multinomial(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "02b13252",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bot(nn.Module):\n",
    "  def __init__(self, encoder, decoder, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "    super().__init__()\n",
    "    if type(temperature)==float or type(temperature)==int:\n",
    "        temperature = lambda pred,*args,temp=temperature: pred/temp\n",
    "    self.temperature = temperature\n",
    "    self.encoder = encoder\n",
    "    self.decoder = decoder\n",
    "    self.chars_from_ids = chars_from_ids\n",
    "    self.ids_from_chars = ids_from_chars\n",
    "    self.softmax = nn.Softmax(-1)\n",
    "\n",
    "  def generate_answer(self, inputs, max_length=500):\n",
    "    self.encoder.eval()\n",
    "    self.decoder.eval()\n",
    "    # Convert strings to token IDs.\n",
    "    input_ids = self.ids_from_chars(inputs)\n",
    "    input_ids = input_ids[None,:]\n",
    "    input_ids = input_ids.to(device)\n",
    "    input_ids = add_positional_info(input_ids)\n",
    "    \n",
    "    # Encode input\n",
    "    encoder_tensor = self.encoder(input_ids,[len(inputs)])\n",
    "\n",
    "    # First Run\n",
    "    input_ids = self.ids_from_chars('\\r')\n",
    "    input_ids = input_ids[None,:]\n",
    "    input_ids = input_ids.to(device)\n",
    "    input_ids = add_positional_info(input_ids)\n",
    "    last_positional_info = input_ids[:,-1:,1]\n",
    "    predicted_logits, states = self.decoder(input_ids,encoder_tensor,[1])\n",
    "    \n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    prediction_position = len(inputs)\n",
    "    predicted_logits = self.temperature(predicted_logits,prediction_position)\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_logits = self.softmax(predicted_logits)\n",
    "    predicted_ids = predicted_logits.multinomial(1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids[0])\n",
    "    predicted_chars = list(predicted_chars)\n",
    "    predicted_chars = predicted_chars[0]\n",
    "    if predicted_chars == ' ':\n",
    "        last_positional_info += 1\n",
    "    elif predicted_chars in ['.','!','?']:\n",
    "        last_positional_info *= 0\n",
    "\n",
    "    run = '\\r' + predicted_chars\n",
    "    for _ in range(max_length):\n",
    "        # Consecutive Run\n",
    "        predicted_ids = torch.stack((predicted_ids,last_positional_info),dim=2)\n",
    "        predicted_logits, states = self.decoder(predicted_ids,encoder_tensor,[1],states)\n",
    "\n",
    "        # Only use the last prediction.\n",
    "        predicted_logits = predicted_logits[:, -1, :]\n",
    "        prediction_position += 1\n",
    "        predicted_logits = self.temperature(predicted_logits,prediction_position)\n",
    "\n",
    "        # Sample the output logits to generate token IDs.\n",
    "        predicted_logits = self.softmax(predicted_logits)\n",
    "        predicted_ids = predicted_logits.multinomial(1)\n",
    "\n",
    "        # Convert from token ids to characters\n",
    "        predicted_chars = self.chars_from_ids(predicted_ids[0])\n",
    "        predicted_chars = list(predicted_chars)\n",
    "        predicted_chars = predicted_chars[0]\n",
    "        \n",
    "        # Update positional info\n",
    "        if predicted_chars == ' ':\n",
    "            last_positional_info += 1\n",
    "        elif predicted_chars in ['.','!','?']:\n",
    "            last_positional_info *= 0\n",
    "\n",
    "        run = run + predicted_chars\n",
    "        if predicted_chars=='\\n':\n",
    "            break\n",
    "    \n",
    "    if run[-1]!='\\n':\n",
    "        run += '\\n'\n",
    "    \n",
    "    return run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cfa4b80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bot = Bot(encoder,decoder,dataset.vec2str,dataset.str2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c7632d87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\rThat world... It was freedom at Bastila.\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot.generate_answer('\\rWhat are you thinking?\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3864cf0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\rThat world... I... I didn't know that of it. I must be able to help you with this, Mandalore! Your father stations may have been thinking for me!\\n\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#temperature = lambda pred,x,b=0: (pred-b)*(pred>b)+(pred-b)*(pred<b)/.33 + b\n",
    "temperature = lambda pred,x,b=.9: pred/(b+(1.-b)/np.sqrt(.1*x+1))\n",
    "bot = Bot(encoder,decoder,dataset.vec2str,dataset.str2vec,temperature=temperature)\n",
    "bot.generate_answer('\\rWhat are you thinking?\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "22d10669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Got any money?\n",
      "\n",
      "\n",
      "Taris guards the prress of its hard significance before. Stock you're quite some stuch things. Brejik may have gained through this plate now.\n",
      "\n",
      "Don't pass the different assassin in my previous use-tomeral.\n",
      "\n",
      "There you are weakness - she still go aheaday after all. You take them for your advancent, yes? My ancient angratunates?\n",
      "\n",
      "There is a Gamorrean of the light side, raving so. Jone does not know much about a Jedi...\n",
      "\n",
      "There used to be a rancor species. Not personally. But how unfortunates takes you now?\n",
      "\n",
      "There you are definitely 8 who will stop me until you do it many thousand beasts that Rorcus Lirs. It claims themselves's running of my allit-tod.\n",
      "\n",
      "There is a docking fee on the last three, though he did not realize them. She was up against the Force... but you have alloed them.\n",
      "\n",
      "There is a planet has been forged better shop to do. The capaties have no cruel something.\n",
      "\n",
      "There is supposed to have a Jedi holocron? Haven's power genetity of our war because we have been charged as the point to Czeraaaa. Our family was the way of this planet.\n",
      "\n",
      "There is a player and start over you, you passed through the restraining proving your strength of planet. Lauto your wisdom, master, your father seem to blame, umxilly discoveries of my sons. These figures are in prison. Light still.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = 'Got any money?'\n",
    "print('Query:',query,end='\\n\\n\\n')\n",
    "for _ in range(10):\n",
    "    print(bot.generate_answer('\\r'+query+'\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced46616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help!\n",
      "Query: Help!\n",
      "\n",
      "\n",
      "We don't allow us to live down. There was more troops for him.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    query = input()\n",
    "    print('Query:',query,end='\\n\\n\\n')\n",
    "    print(bot.generate_answer('\\r'+query+'\\n'),end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c061a9b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
